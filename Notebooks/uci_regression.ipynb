{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as ticker\n",
    "import torch.utils.data as data_utils\n",
    "import os\n",
    "import warnings \n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Prior_optimization.gpr import GPR\n",
    "from Prior_optimization import kernels, mean_functions, priors\n",
    "from Networks.gaussian_reparam_mlp import GaussianMLPReparameterization\n",
    "from Networks.factorized_gaussian_reparam_mlp import FactorizedGaussianMLPReparameterization\n",
    "from Networks.mlp import MLP\n",
    "from Samplers.likelihoods import LikGaussian\n",
    "from Prior_optimization.priors import FixedGaussianPrior, OptimGaussianPrior\n",
    "from Utilities.rand_generators import MeasureSetGenerator, GridGenerator\n",
    "from Utilities.normalization import normalize_data\n",
    "from Utilities.exp_utils import get_input_range\n",
    "from Metrics.sampling import compute_rhat_regression\n",
    "from Metrics import uncertainty as uncertainty_metrics\n",
    "from Networks.regression_net import RegressionNet\n",
    "from Networks.mlp_masked import MLPMasked\n",
    "from Networks.regression_net_masked import RegressionNetMasked\n",
    "from Prior_optimization.sinkhorn_optimisation import SinkhornMapper\n",
    "from Utilities import util\n",
    "from Utilities.priors import LogNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "util.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network architecture\n",
    "n_units = 100\n",
    "n_hidden = 1\n",
    "activation_fn = \"tanh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configurations\n",
    "n_splits = 10\n",
    "dataset = \"boston\"\n",
    "data_dir = \"./data/uci\"\n",
    "noise_var = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Optimized Gaussian Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"./exp/uci/optim_gaussian\"\n",
    "util.ensure_dir(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Optimize the prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 200  # Number of iteterations of Wasserstein optimization\n",
    "lr = 0.05        # The learning rate\n",
    "n_samples = 128  # The mini-batch size\n",
    "lambd_sd = torch.tensor([2.]) # Vetcor of penalization terms\n",
    "D = 3            # Depth for pruning of standard deviations\n",
    "num_samples = 30\n",
    "num_iters_sd = 5000\n",
    "print_every = 100\n",
    "num_burn_in_steps = 200\n",
    "n_data = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading split 1 of boston dataset\n",
      ">>> Iteration #   1: Sinkhorn Dist 40.8609 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 100: Sinkhorn Dist 43.6713 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 200: Sinkhorn Dist 23.7334 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 300: Sinkhorn Dist 28.3756 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 400: Sinkhorn Dist 30.2864 Number of pruned stochastic weights: 5\n",
      ">>> Iteration # 500: Sinkhorn Dist 34.7831 Number of pruned stochastic weights: 7\n",
      ">>> Iteration # 600: Sinkhorn Dist 18.8343 Number of pruned stochastic weights: 12\n",
      ">>> Iteration # 700: Sinkhorn Dist 25.3957 Number of pruned stochastic weights: 17\n",
      ">>> Iteration # 800: Sinkhorn Dist 11.7751 Number of pruned stochastic weights: 24\n",
      ">>> Iteration # 900: Sinkhorn Dist 11.6134 Number of pruned stochastic weights: 34\n",
      ">>> Iteration # 1000: Sinkhorn Dist 11.3621 Number of pruned stochastic weights: 55\n",
      ">>> Iteration # 1100: Sinkhorn Dist 20.9282 Number of pruned stochastic weights: 72\n",
      ">>> Iteration # 1200: Sinkhorn Dist 9.7489 Number of pruned stochastic weights: 94\n",
      ">>> Iteration # 1300: Sinkhorn Dist 20.2688 Number of pruned stochastic weights: 120\n",
      ">>> Iteration # 1400: Sinkhorn Dist 10.1293 Number of pruned stochastic weights: 148\n",
      ">>> Iteration # 1500: Sinkhorn Dist 10.7719 Number of pruned stochastic weights: 188\n",
      ">>> Iteration # 1600: Sinkhorn Dist 5.8092 Number of pruned stochastic weights: 217\n",
      ">>> Iteration # 1700: Sinkhorn Dist 15.4340 Number of pruned stochastic weights: 247\n",
      ">>> Iteration # 1800: Sinkhorn Dist 8.5449 Number of pruned stochastic weights: 279\n",
      ">>> Iteration # 1900: Sinkhorn Dist 9.8714 Number of pruned stochastic weights: 305\n",
      ">>> Iteration # 2000: Sinkhorn Dist 7.6119 Number of pruned stochastic weights: 327\n",
      ">>> Iteration # 2100: Sinkhorn Dist 7.4385 Number of pruned stochastic weights: 362\n",
      ">>> Iteration # 2200: Sinkhorn Dist 7.3589 Number of pruned stochastic weights: 380\n",
      ">>> Iteration # 2300: Sinkhorn Dist 6.7475 Number of pruned stochastic weights: 397\n",
      ">>> Iteration # 2400: Sinkhorn Dist 15.6799 Number of pruned stochastic weights: 419\n",
      ">>> Iteration # 2500: Sinkhorn Dist 8.1831 Number of pruned stochastic weights: 440\n",
      "--------------------------------------------------------------------------------\n",
      "Loading split 2 of boston dataset\n",
      ">>> Iteration #   1: Sinkhorn Dist 39.6246 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 100: Sinkhorn Dist 26.8486 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 200: Sinkhorn Dist 42.8080 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 300: Sinkhorn Dist 25.4423 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 400: Sinkhorn Dist 32.8120 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 500: Sinkhorn Dist 24.3727 Number of pruned stochastic weights: 7\n",
      ">>> Iteration # 600: Sinkhorn Dist 16.1435 Number of pruned stochastic weights: 10\n",
      ">>> Iteration # 700: Sinkhorn Dist 23.8032 Number of pruned stochastic weights: 14\n",
      ">>> Iteration # 800: Sinkhorn Dist 10.8322 Number of pruned stochastic weights: 24\n",
      ">>> Iteration # 900: Sinkhorn Dist 24.7776 Number of pruned stochastic weights: 38\n",
      ">>> Iteration # 1000: Sinkhorn Dist 18.1182 Number of pruned stochastic weights: 52\n",
      ">>> Iteration # 1100: Sinkhorn Dist 8.8705 Number of pruned stochastic weights: 74\n",
      ">>> Iteration # 1200: Sinkhorn Dist 9.7090 Number of pruned stochastic weights: 108\n",
      ">>> Iteration # 1300: Sinkhorn Dist 12.1463 Number of pruned stochastic weights: 127\n",
      ">>> Iteration # 1400: Sinkhorn Dist 8.5298 Number of pruned stochastic weights: 157\n",
      ">>> Iteration # 1500: Sinkhorn Dist 12.5041 Number of pruned stochastic weights: 181\n",
      ">>> Iteration # 1600: Sinkhorn Dist 9.0207 Number of pruned stochastic weights: 206\n",
      ">>> Iteration # 1700: Sinkhorn Dist 10.8397 Number of pruned stochastic weights: 233\n",
      ">>> Iteration # 1800: Sinkhorn Dist 10.4457 Number of pruned stochastic weights: 247\n",
      ">>> Iteration # 1900: Sinkhorn Dist 7.3056 Number of pruned stochastic weights: 271\n",
      ">>> Iteration # 2000: Sinkhorn Dist 7.0859 Number of pruned stochastic weights: 299\n",
      ">>> Iteration # 2100: Sinkhorn Dist 9.4095 Number of pruned stochastic weights: 324\n",
      ">>> Iteration # 2200: Sinkhorn Dist 10.9508 Number of pruned stochastic weights: 344\n",
      ">>> Iteration # 2300: Sinkhorn Dist 11.8140 Number of pruned stochastic weights: 367\n",
      ">>> Iteration # 2400: Sinkhorn Dist 7.5036 Number of pruned stochastic weights: 394\n",
      ">>> Iteration # 2500: Sinkhorn Dist 5.6568 Number of pruned stochastic weights: 412\n",
      "--------------------------------------------------------------------------------\n",
      "Loading split 3 of boston dataset\n",
      ">>> Iteration #   1: Sinkhorn Dist 47.5308 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 100: Sinkhorn Dist 18.4564 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 200: Sinkhorn Dist 35.4815 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 300: Sinkhorn Dist 34.4731 Number of pruned stochastic weights: 3\n",
      ">>> Iteration # 400: Sinkhorn Dist 21.3686 Number of pruned stochastic weights: 3\n",
      ">>> Iteration # 500: Sinkhorn Dist 17.9652 Number of pruned stochastic weights: 7\n",
      ">>> Iteration # 600: Sinkhorn Dist 18.5184 Number of pruned stochastic weights: 13\n",
      ">>> Iteration # 700: Sinkhorn Dist 15.7811 Number of pruned stochastic weights: 28\n",
      ">>> Iteration # 800: Sinkhorn Dist 7.6792 Number of pruned stochastic weights: 37\n",
      ">>> Iteration # 900: Sinkhorn Dist 12.2538 Number of pruned stochastic weights: 52\n",
      ">>> Iteration # 1000: Sinkhorn Dist 15.6242 Number of pruned stochastic weights: 65\n",
      ">>> Iteration # 1100: Sinkhorn Dist 15.7705 Number of pruned stochastic weights: 85\n",
      ">>> Iteration # 1200: Sinkhorn Dist 22.0927 Number of pruned stochastic weights: 127\n",
      ">>> Iteration # 1300: Sinkhorn Dist 12.4324 Number of pruned stochastic weights: 149\n",
      ">>> Iteration # 1400: Sinkhorn Dist 20.6668 Number of pruned stochastic weights: 176\n",
      ">>> Iteration # 1500: Sinkhorn Dist 10.1368 Number of pruned stochastic weights: 197\n",
      ">>> Iteration # 1600: Sinkhorn Dist 20.8585 Number of pruned stochastic weights: 218\n",
      ">>> Iteration # 1700: Sinkhorn Dist 14.7381 Number of pruned stochastic weights: 240\n",
      ">>> Iteration # 1800: Sinkhorn Dist 10.4614 Number of pruned stochastic weights: 261\n",
      ">>> Iteration # 1900: Sinkhorn Dist 10.5680 Number of pruned stochastic weights: 284\n",
      ">>> Iteration # 2000: Sinkhorn Dist 12.7705 Number of pruned stochastic weights: 304\n",
      ">>> Iteration # 2100: Sinkhorn Dist 7.4385 Number of pruned stochastic weights: 324\n",
      ">>> Iteration # 2200: Sinkhorn Dist 10.2969 Number of pruned stochastic weights: 347\n",
      ">>> Iteration # 2300: Sinkhorn Dist 8.1646 Number of pruned stochastic weights: 371\n",
      ">>> Iteration # 2400: Sinkhorn Dist 8.4913 Number of pruned stochastic weights: 394\n",
      ">>> Iteration # 2500: Sinkhorn Dist 10.7685 Number of pruned stochastic weights: 411\n",
      "--------------------------------------------------------------------------------\n",
      "Loading split 4 of boston dataset\n",
      ">>> Iteration #   1: Sinkhorn Dist 28.6059 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 100: Sinkhorn Dist 23.1582 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 200: Sinkhorn Dist 26.2572 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 300: Sinkhorn Dist 30.1731 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 400: Sinkhorn Dist 18.8226 Number of pruned stochastic weights: 7\n",
      ">>> Iteration # 500: Sinkhorn Dist 24.9399 Number of pruned stochastic weights: 9\n",
      ">>> Iteration # 600: Sinkhorn Dist 22.0599 Number of pruned stochastic weights: 16\n",
      ">>> Iteration # 700: Sinkhorn Dist 9.8667 Number of pruned stochastic weights: 26\n",
      ">>> Iteration # 800: Sinkhorn Dist 25.1181 Number of pruned stochastic weights: 41\n",
      ">>> Iteration # 900: Sinkhorn Dist 26.2703 Number of pruned stochastic weights: 59\n",
      ">>> Iteration # 1000: Sinkhorn Dist 17.2750 Number of pruned stochastic weights: 85\n",
      ">>> Iteration # 1100: Sinkhorn Dist 7.6741 Number of pruned stochastic weights: 105\n",
      ">>> Iteration # 1200: Sinkhorn Dist 39.0884 Number of pruned stochastic weights: 131\n",
      ">>> Iteration # 1300: Sinkhorn Dist 14.6562 Number of pruned stochastic weights: 149\n",
      ">>> Iteration # 1400: Sinkhorn Dist 9.2198 Number of pruned stochastic weights: 172\n",
      ">>> Iteration # 1500: Sinkhorn Dist 8.3048 Number of pruned stochastic weights: 193\n",
      ">>> Iteration # 1600: Sinkhorn Dist 21.4917 Number of pruned stochastic weights: 214\n",
      ">>> Iteration # 1700: Sinkhorn Dist 9.4614 Number of pruned stochastic weights: 234\n",
      ">>> Iteration # 1800: Sinkhorn Dist 6.6771 Number of pruned stochastic weights: 264\n",
      ">>> Iteration # 1900: Sinkhorn Dist 6.7342 Number of pruned stochastic weights: 279\n",
      ">>> Iteration # 2000: Sinkhorn Dist 5.8459 Number of pruned stochastic weights: 295\n",
      ">>> Iteration # 2100: Sinkhorn Dist 8.6067 Number of pruned stochastic weights: 310\n",
      ">>> Iteration # 2200: Sinkhorn Dist 21.1625 Number of pruned stochastic weights: 336\n",
      ">>> Iteration # 2300: Sinkhorn Dist 20.2756 Number of pruned stochastic weights: 357\n",
      ">>> Iteration # 2400: Sinkhorn Dist 12.5084 Number of pruned stochastic weights: 375\n",
      ">>> Iteration # 2500: Sinkhorn Dist 9.9314 Number of pruned stochastic weights: 384\n",
      "--------------------------------------------------------------------------------\n",
      "Loading split 5 of boston dataset\n",
      ">>> Iteration #   1: Sinkhorn Dist 29.6495 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 100: Sinkhorn Dist 34.6699 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 200: Sinkhorn Dist 28.8048 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 300: Sinkhorn Dist 24.9920 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 400: Sinkhorn Dist 31.3969 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 500: Sinkhorn Dist 24.5341 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 600: Sinkhorn Dist 22.3799 Number of pruned stochastic weights: 9\n",
      ">>> Iteration # 700: Sinkhorn Dist 29.7125 Number of pruned stochastic weights: 16\n",
      ">>> Iteration # 800: Sinkhorn Dist 19.9458 Number of pruned stochastic weights: 26\n",
      ">>> Iteration # 900: Sinkhorn Dist 16.4803 Number of pruned stochastic weights: 44\n",
      ">>> Iteration # 1000: Sinkhorn Dist 22.1306 Number of pruned stochastic weights: 58\n",
      ">>> Iteration # 1100: Sinkhorn Dist 5.8042 Number of pruned stochastic weights: 68\n",
      ">>> Iteration # 1200: Sinkhorn Dist 11.4445 Number of pruned stochastic weights: 91\n",
      ">>> Iteration # 1300: Sinkhorn Dist 13.1547 Number of pruned stochastic weights: 125\n",
      ">>> Iteration # 1400: Sinkhorn Dist 10.5271 Number of pruned stochastic weights: 147\n",
      ">>> Iteration # 1500: Sinkhorn Dist 9.2050 Number of pruned stochastic weights: 179\n",
      ">>> Iteration # 1600: Sinkhorn Dist 18.1652 Number of pruned stochastic weights: 197\n",
      ">>> Iteration # 1700: Sinkhorn Dist 24.3698 Number of pruned stochastic weights: 217\n",
      ">>> Iteration # 1800: Sinkhorn Dist 17.6628 Number of pruned stochastic weights: 228\n",
      ">>> Iteration # 1900: Sinkhorn Dist 13.1833 Number of pruned stochastic weights: 245\n",
      ">>> Iteration # 2000: Sinkhorn Dist 17.2670 Number of pruned stochastic weights: 261\n",
      ">>> Iteration # 2100: Sinkhorn Dist 10.0553 Number of pruned stochastic weights: 281\n",
      ">>> Iteration # 2200: Sinkhorn Dist 6.2785 Number of pruned stochastic weights: 300\n",
      ">>> Iteration # 2300: Sinkhorn Dist 7.1107 Number of pruned stochastic weights: 318\n",
      ">>> Iteration # 2400: Sinkhorn Dist 6.5127 Number of pruned stochastic weights: 331\n",
      ">>> Iteration # 2500: Sinkhorn Dist 7.6167 Number of pruned stochastic weights: 339\n",
      "--------------------------------------------------------------------------------\n",
      "Loading split 6 of boston dataset\n",
      ">>> Iteration #   1: Sinkhorn Dist 45.4009 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 100: Sinkhorn Dist 34.5686 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 200: Sinkhorn Dist 28.7943 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 300: Sinkhorn Dist 26.3690 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 400: Sinkhorn Dist 16.5037 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 500: Sinkhorn Dist 34.1627 Number of pruned stochastic weights: 5\n",
      ">>> Iteration # 600: Sinkhorn Dist 13.8708 Number of pruned stochastic weights: 8\n",
      ">>> Iteration # 700: Sinkhorn Dist 34.4688 Number of pruned stochastic weights: 10\n",
      ">>> Iteration # 800: Sinkhorn Dist 14.2658 Number of pruned stochastic weights: 17\n",
      ">>> Iteration # 900: Sinkhorn Dist 6.7727 Number of pruned stochastic weights: 24\n",
      ">>> Iteration # 1000: Sinkhorn Dist 7.7865 Number of pruned stochastic weights: 35\n",
      ">>> Iteration # 1100: Sinkhorn Dist 9.4770 Number of pruned stochastic weights: 51\n",
      ">>> Iteration # 1200: Sinkhorn Dist 9.7535 Number of pruned stochastic weights: 74\n",
      ">>> Iteration # 1300: Sinkhorn Dist 8.6227 Number of pruned stochastic weights: 96\n",
      ">>> Iteration # 1400: Sinkhorn Dist 18.6952 Number of pruned stochastic weights: 111\n",
      ">>> Iteration # 1500: Sinkhorn Dist 9.2010 Number of pruned stochastic weights: 140\n",
      ">>> Iteration # 1600: Sinkhorn Dist 10.9214 Number of pruned stochastic weights: 157\n",
      ">>> Iteration # 1700: Sinkhorn Dist 8.1384 Number of pruned stochastic weights: 180\n",
      ">>> Iteration # 1800: Sinkhorn Dist 8.3873 Number of pruned stochastic weights: 202\n",
      ">>> Iteration # 1900: Sinkhorn Dist 21.2343 Number of pruned stochastic weights: 220\n",
      ">>> Iteration # 2000: Sinkhorn Dist 12.0245 Number of pruned stochastic weights: 235\n",
      ">>> Iteration # 2100: Sinkhorn Dist 8.0878 Number of pruned stochastic weights: 254\n",
      ">>> Iteration # 2200: Sinkhorn Dist 5.4746 Number of pruned stochastic weights: 265\n",
      ">>> Iteration # 2300: Sinkhorn Dist 13.7357 Number of pruned stochastic weights: 280\n",
      ">>> Iteration # 2400: Sinkhorn Dist 10.6259 Number of pruned stochastic weights: 306\n",
      ">>> Iteration # 2500: Sinkhorn Dist 10.2207 Number of pruned stochastic weights: 335\n",
      "--------------------------------------------------------------------------------\n",
      "Loading split 7 of boston dataset\n",
      ">>> Iteration #   1: Sinkhorn Dist 37.2949 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 100: Sinkhorn Dist 39.3517 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 200: Sinkhorn Dist 25.8223 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 300: Sinkhorn Dist 19.4069 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 400: Sinkhorn Dist 16.9773 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 500: Sinkhorn Dist 39.5460 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 600: Sinkhorn Dist 23.4673 Number of pruned stochastic weights: 5\n",
      ">>> Iteration # 700: Sinkhorn Dist 19.1487 Number of pruned stochastic weights: 9\n",
      ">>> Iteration # 800: Sinkhorn Dist 15.1130 Number of pruned stochastic weights: 17\n",
      ">>> Iteration # 900: Sinkhorn Dist 18.7537 Number of pruned stochastic weights: 29\n",
      ">>> Iteration # 1000: Sinkhorn Dist 13.7124 Number of pruned stochastic weights: 49\n",
      ">>> Iteration # 1100: Sinkhorn Dist 15.6535 Number of pruned stochastic weights: 66\n",
      ">>> Iteration # 1200: Sinkhorn Dist 13.1331 Number of pruned stochastic weights: 80\n",
      ">>> Iteration # 1300: Sinkhorn Dist 6.0730 Number of pruned stochastic weights: 110\n",
      ">>> Iteration # 1400: Sinkhorn Dist 10.6471 Number of pruned stochastic weights: 138\n",
      ">>> Iteration # 1500: Sinkhorn Dist 8.8405 Number of pruned stochastic weights: 168\n",
      ">>> Iteration # 1600: Sinkhorn Dist 8.0011 Number of pruned stochastic weights: 191\n",
      ">>> Iteration # 1700: Sinkhorn Dist 6.3278 Number of pruned stochastic weights: 227\n",
      ">>> Iteration # 1800: Sinkhorn Dist 16.8820 Number of pruned stochastic weights: 249\n",
      ">>> Iteration # 1900: Sinkhorn Dist 10.3591 Number of pruned stochastic weights: 265\n",
      ">>> Iteration # 2000: Sinkhorn Dist 9.4775 Number of pruned stochastic weights: 288\n",
      ">>> Iteration # 2100: Sinkhorn Dist 12.5500 Number of pruned stochastic weights: 315\n",
      ">>> Iteration # 2200: Sinkhorn Dist 9.7413 Number of pruned stochastic weights: 341\n",
      ">>> Iteration # 2300: Sinkhorn Dist 5.9361 Number of pruned stochastic weights: 361\n",
      ">>> Iteration # 2400: Sinkhorn Dist 18.7808 Number of pruned stochastic weights: 383\n",
      ">>> Iteration # 2500: Sinkhorn Dist 7.1440 Number of pruned stochastic weights: 408\n",
      "--------------------------------------------------------------------------------\n",
      "Loading split 8 of boston dataset\n",
      ">>> Iteration #   1: Sinkhorn Dist 69.1118 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 100: Sinkhorn Dist 37.5564 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 200: Sinkhorn Dist 37.1071 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 300: Sinkhorn Dist 27.2637 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 400: Sinkhorn Dist 45.4454 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 500: Sinkhorn Dist 26.1812 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 600: Sinkhorn Dist 28.4028 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 700: Sinkhorn Dist 20.7993 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 800: Sinkhorn Dist 30.1126 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 900: Sinkhorn Dist 13.1447 Number of pruned stochastic weights: 8\n",
      ">>> Iteration # 1000: Sinkhorn Dist 7.6262 Number of pruned stochastic weights: 16\n",
      ">>> Iteration # 1100: Sinkhorn Dist 25.4794 Number of pruned stochastic weights: 30\n",
      ">>> Iteration # 1200: Sinkhorn Dist 11.4432 Number of pruned stochastic weights: 49\n",
      ">>> Iteration # 1300: Sinkhorn Dist 6.8413 Number of pruned stochastic weights: 69\n",
      ">>> Iteration # 1400: Sinkhorn Dist 8.8007 Number of pruned stochastic weights: 84\n",
      ">>> Iteration # 1500: Sinkhorn Dist 10.3642 Number of pruned stochastic weights: 106\n",
      ">>> Iteration # 1600: Sinkhorn Dist 5.2195 Number of pruned stochastic weights: 126\n",
      ">>> Iteration # 1700: Sinkhorn Dist 6.8280 Number of pruned stochastic weights: 149\n",
      ">>> Iteration # 1800: Sinkhorn Dist 3.5391 Number of pruned stochastic weights: 171\n",
      ">>> Iteration # 1900: Sinkhorn Dist 8.5163 Number of pruned stochastic weights: 197\n",
      ">>> Iteration # 2000: Sinkhorn Dist 8.9689 Number of pruned stochastic weights: 209\n",
      ">>> Iteration # 2100: Sinkhorn Dist 10.2850 Number of pruned stochastic weights: 224\n",
      ">>> Iteration # 2200: Sinkhorn Dist 10.9885 Number of pruned stochastic weights: 245\n",
      ">>> Iteration # 2300: Sinkhorn Dist 7.5086 Number of pruned stochastic weights: 276\n",
      ">>> Iteration # 2400: Sinkhorn Dist 14.3382 Number of pruned stochastic weights: 298\n",
      ">>> Iteration # 2500: Sinkhorn Dist 8.5678 Number of pruned stochastic weights: 314\n",
      "--------------------------------------------------------------------------------\n",
      "Loading split 9 of boston dataset\n",
      ">>> Iteration #   1: Sinkhorn Dist 57.7615 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 100: Sinkhorn Dist 42.4114 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 200: Sinkhorn Dist 32.2355 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 300: Sinkhorn Dist 25.0357 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 400: Sinkhorn Dist 34.7279 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 500: Sinkhorn Dist 39.5903 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 600: Sinkhorn Dist 17.9413 Number of pruned stochastic weights: 8\n",
      ">>> Iteration # 700: Sinkhorn Dist 18.5815 Number of pruned stochastic weights: 11\n",
      ">>> Iteration # 800: Sinkhorn Dist 17.6087 Number of pruned stochastic weights: 17\n",
      ">>> Iteration # 900: Sinkhorn Dist 19.8199 Number of pruned stochastic weights: 22\n",
      ">>> Iteration # 1000: Sinkhorn Dist 21.3226 Number of pruned stochastic weights: 37\n",
      ">>> Iteration # 1100: Sinkhorn Dist 18.2852 Number of pruned stochastic weights: 54\n",
      ">>> Iteration # 1200: Sinkhorn Dist 28.0447 Number of pruned stochastic weights: 75\n",
      ">>> Iteration # 1300: Sinkhorn Dist 9.3066 Number of pruned stochastic weights: 95\n",
      ">>> Iteration # 1400: Sinkhorn Dist 8.3774 Number of pruned stochastic weights: 114\n",
      ">>> Iteration # 1500: Sinkhorn Dist 36.6642 Number of pruned stochastic weights: 135\n",
      ">>> Iteration # 1600: Sinkhorn Dist 10.9210 Number of pruned stochastic weights: 156\n",
      ">>> Iteration # 1700: Sinkhorn Dist 21.1646 Number of pruned stochastic weights: 174\n",
      ">>> Iteration # 1800: Sinkhorn Dist 9.0570 Number of pruned stochastic weights: 194\n",
      ">>> Iteration # 1900: Sinkhorn Dist 14.4907 Number of pruned stochastic weights: 218\n",
      ">>> Iteration # 2000: Sinkhorn Dist 11.7368 Number of pruned stochastic weights: 238\n",
      ">>> Iteration # 2100: Sinkhorn Dist 10.8574 Number of pruned stochastic weights: 259\n",
      ">>> Iteration # 2200: Sinkhorn Dist 5.4444 Number of pruned stochastic weights: 271\n",
      ">>> Iteration # 2300: Sinkhorn Dist 8.7314 Number of pruned stochastic weights: 300\n",
      ">>> Iteration # 2400: Sinkhorn Dist 12.5576 Number of pruned stochastic weights: 327\n",
      ">>> Iteration # 2500: Sinkhorn Dist 7.3748 Number of pruned stochastic weights: 344\n",
      "--------------------------------------------------------------------------------\n",
      "Loading split 10 of boston dataset\n",
      ">>> Iteration #   1: Sinkhorn Dist 65.1754 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 100: Sinkhorn Dist 34.9054 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 200: Sinkhorn Dist 27.5438 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 300: Sinkhorn Dist 33.3052 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 400: Sinkhorn Dist 23.6263 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 500: Sinkhorn Dist 29.9012 Number of pruned stochastic weights: 3\n",
      ">>> Iteration # 600: Sinkhorn Dist 24.8193 Number of pruned stochastic weights: 5\n",
      ">>> Iteration # 700: Sinkhorn Dist 18.5276 Number of pruned stochastic weights: 14\n",
      ">>> Iteration # 800: Sinkhorn Dist 12.1724 Number of pruned stochastic weights: 17\n",
      ">>> Iteration # 900: Sinkhorn Dist 12.9577 Number of pruned stochastic weights: 29\n",
      ">>> Iteration # 1000: Sinkhorn Dist 13.0794 Number of pruned stochastic weights: 42\n",
      ">>> Iteration # 1100: Sinkhorn Dist 12.8237 Number of pruned stochastic weights: 69\n",
      ">>> Iteration # 1200: Sinkhorn Dist 6.0305 Number of pruned stochastic weights: 85\n",
      ">>> Iteration # 1300: Sinkhorn Dist 8.7992 Number of pruned stochastic weights: 101\n",
      ">>> Iteration # 1400: Sinkhorn Dist 15.0945 Number of pruned stochastic weights: 124\n",
      ">>> Iteration # 1500: Sinkhorn Dist 9.1116 Number of pruned stochastic weights: 149\n",
      ">>> Iteration # 1600: Sinkhorn Dist 37.4059 Number of pruned stochastic weights: 172\n",
      ">>> Iteration # 1700: Sinkhorn Dist 5.5061 Number of pruned stochastic weights: 195\n",
      ">>> Iteration # 1800: Sinkhorn Dist 25.6319 Number of pruned stochastic weights: 215\n",
      ">>> Iteration # 1900: Sinkhorn Dist 13.3820 Number of pruned stochastic weights: 235\n",
      ">>> Iteration # 2000: Sinkhorn Dist 9.6969 Number of pruned stochastic weights: 256\n",
      ">>> Iteration # 2100: Sinkhorn Dist 16.7340 Number of pruned stochastic weights: 279\n",
      ">>> Iteration # 2200: Sinkhorn Dist 9.5721 Number of pruned stochastic weights: 307\n",
      ">>> Iteration # 2300: Sinkhorn Dist 9.7714 Number of pruned stochastic weights: 328\n",
      ">>> Iteration # 2400: Sinkhorn Dist 14.0582 Number of pruned stochastic weights: 352\n",
      ">>> Iteration # 2500: Sinkhorn Dist 8.3615 Number of pruned stochastic weights: 378\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "masks_list = []\n",
    "for split_id in range(n_splits):\n",
    "    print(\"Loading split {} of {} dataset\".format(split_id+1, dataset))\n",
    "    # Load the dataset\n",
    "    saved_dir = os.path.join(out_dir, str(split_id))\n",
    "    X_train, y_train, X_test, y_test = util.load_uci_data(\n",
    "            data_dir, split_id, dataset)\n",
    "    X_train_, y_train_, X_test_, y_test_, y_mean, y_std = normalize_data(\n",
    "            X_train, y_train, X_test, y_test)\n",
    "    x_min, x_max = get_input_range(X_train_, X_test_)\n",
    "    input_dim, output_dim = int(X_train.shape[-1]), 1\n",
    "    \n",
    "    # Initialize the measurement set generator\n",
    "    rand_generator = MeasureSetGenerator(X_train_, x_min, x_max, 0.7)\n",
    "    \n",
    "    # Initialize the mean and covariance function of the target hierarchical GP prior\n",
    "    mean = mean_functions.Zero()\n",
    "    \n",
    "    lengthscale = math.sqrt(2. * input_dim)\n",
    "    variance = 1.\n",
    "    kernel = kernels.RBF(input_dim=input_dim,\n",
    "                         lengthscales=torch.tensor([lengthscale], dtype=torch.double),\n",
    "                         variance=torch.tensor([variance], dtype=torch.double), ARD=True)\n",
    "\n",
    "    # Place hyper-priors on lengthscales and variances\n",
    "    kernel.lengthscales.prior = LogNormal(\n",
    "            torch.ones([input_dim]) * math.log(lengthscale),\n",
    "            torch.ones([input_dim]) * 1.)\n",
    "    kernel.variance.prior = LogNormal(\n",
    "            torch.ones([1]) * 0.1,\n",
    "            torch.ones([1]) * 1.)\n",
    "        \n",
    "    # Initialize the GP model\n",
    "    gp = GPR(X=torch.from_numpy(X_train_), Y=torch.from_numpy(y_train_).reshape([-1, 1]),\n",
    "             kern=kernel, mean_function=mean)\n",
    "    gp.likelihood.variance.set(noise_var)\n",
    "    \n",
    "    # Initialize tunable MLP prior\n",
    "    hidden_dims = [n_units] * n_hidden\n",
    "    mlp_reparam = FactorizedGaussianMLPReparameterization(input_dim, output_dim,\n",
    "        hidden_dims, D = D, activation_fn=activation_fn, scaled_variance=True)\n",
    "    # Perform optimization\n",
    "    mapper = SinkhornMapper(out_dir=saved_dir)\n",
    "    y_train_tensor = torch.tensor(y_train)  # Assumes y is 1D: (batch_size,)\n",
    "    y_train = y_train_tensor[:, None, None].expand(-1, num_samples, 1)\n",
    "    p_hist, loss_hist = mapper.optimize_sparse(net = mlp_reparam, gp = gp, data_generator = rand_generator, \n",
    "            lambd= lambd_sd, n_data = n_data, num_iters = num_iters_sd, print_every=print_every, output_dim = 1, \n",
    "            D= D, X_train = torch.tensor(X_train), y_train= y_train, n_samples = num_samples)\n",
    "    path = os.path.join(saved_dir, \"loss_values.log\")\n",
    "    if not os.path.isfile(saved_dir):\n",
    "        os.makedirs(saved_dir, exist_ok=True)\n",
    "    np.savetxt(path, loss_hist, fmt='%.6e')\n",
    "    path = os.path.join(saved_dir, \"pruned_values.log\")\n",
    "    if not os.path.isfile(saved_dir):\n",
    "        os.makedirs(saved_dir, exist_ok=True)\n",
    "    np.savetxt(path, p_hist, fmt='%.6e')\n",
    "    print(\"----\" * 20)\n",
    "    masks_list.append(mlp_reparam.get_det_masks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x350 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the convergence\n",
    "lossdist_data = []\n",
    "pruned_vals_data = []\n",
    "for i in range(0, n_splits):\n",
    "    lossdist_file = os.path.join(out_dir, str(i), \"loss_values.log\")\n",
    "    pruned_vals_file = os.path.join(out_dir, str(i), \"pruned_values.log\")\n",
    "    lossdist_data.append(np.loadtxt(lossdist_file))\n",
    "    pruned_vals_data.append(np.loadtxt(pruned_vals_file))\n",
    "\n",
    "lossdist_vals = np.stack(lossdist_data)\n",
    "pruned_vals = np.stack(pruned_vals_data)\n",
    "indices = np.arange(num_iters_sd)[::100]\n",
    "mean_loss = lossdist_vals.mean(0)\n",
    "std_loss = lossdist_vals.std(0)\n",
    "mean_pruned = pruned_vals.mean(0)\n",
    "std_pruned = pruned_vals.std(0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 3.5))  \n",
    "axes[0].plot(indices, mean_pruned[indices], \"ko\", ms=4)\n",
    "axes[0].fill_between(indices, mean_pruned[indices] - std_pruned[indices],\n",
    "                 mean_pruned[indices] + std_pruned[indices], alpha=0.18, color=\"k\")\n",
    "axes[0].set_ylabel(\"Number pruned weights\")\n",
    "axes[0].set_xlabel(\"Iteration\")\n",
    "axes[0].yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "axes[1].plot(indices, mean_loss[indices], \"-ko\", ms=4)\n",
    "axes[1].fill_between(indices, mean_loss[indices]-std_loss[indices],\n",
    "                     mean_loss[indices]+std_loss[indices], alpha = 0.18, color = \"k\")\n",
    "axes[1].set_ylabel(\"Sinkhorn Distance\")\n",
    "axes[1].set_xlabel(\"Iteration\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Posterior Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Prevention from Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_configs = {\n",
    "    \"batch_size\": 32,                 # Mini-batch size\n",
    "    \"num_samples\": 40,                # Total number of samples for each chain \n",
    "    \"n_discarded\": 10,                # Number of the first samples to be discared for each chain\n",
    "    \"num_burn_in_steps\": 2000,         # Number of burn-in steps\n",
    "    \"keep_every\": 2000,                # Thinning interval\n",
    "    \"lr\": 1e-2,                       # Step size\n",
    "    \"num_chains\": 4,                  # Number of chains\n",
    "    \"mdecay\": 1e-2,                   # Momentum coefficient\n",
    "    \"print_every_n_samples\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading split 1 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0941e+00 RMSE = 8.6514e-01 \n",
      "Samples #    10 : NLL =  2.0876e+00 RMSE = 7.1720e-01 \n",
      "Samples #    15 : NLL =  2.0840e+00 RMSE = 6.3318e-01 \n",
      "Samples #    20 : NLL =  2.0820e+00 RMSE = 5.8438e-01 \n",
      "Samples #    25 : NLL =  2.0791e+00 RMSE = 5.4801e-01 \n",
      "Samples #    30 : NLL =  2.0765e+00 RMSE = 5.2505e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0796e+00 RMSE = 5.3079e-01 \n",
      "Samples #    40 : NLL =  2.0813e+00 RMSE = 5.3515e-01 \n",
      "Samples #    45 : NLL =  2.0830e+00 RMSE = 5.3898e-01 \n",
      "Samples #    50 : NLL =  2.0840e+00 RMSE = 5.4111e-01 \n",
      "Samples #    55 : NLL =  2.0844e+00 RMSE = 5.4900e-01 \n",
      "Samples #    60 : NLL =  2.0849e+00 RMSE = 5.5475e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.0863e+00 RMSE = 5.5440e-01 \n",
      "Samples #    70 : NLL =  2.0869e+00 RMSE = 5.5442e-01 \n",
      "Samples #    75 : NLL =  2.0872e+00 RMSE = 5.5306e-01 \n",
      "Samples #    80 : NLL =  2.0878e+00 RMSE = 5.5185e-01 \n",
      "Samples #    85 : NLL =  2.0878e+00 RMSE = 5.5121e-01 \n",
      "Samples #    90 : NLL =  2.0872e+00 RMSE = 5.4814e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.0884e+00 RMSE = 5.4750e-01 \n",
      "Samples #   100 : NLL =  2.0890e+00 RMSE = 5.4686e-01 \n",
      "Samples #   105 : NLL =  2.0891e+00 RMSE = 5.4312e-01 \n",
      "Samples #   110 : NLL =  2.0889e+00 RMSE = 5.3938e-01 \n",
      "Samples #   115 : NLL =  2.0887e+00 RMSE = 5.3441e-01 \n",
      "Samples #   120 : NLL =  2.0884e+00 RMSE = 5.3031e-01 \n",
      "R-hat: mean 1.2417 std 0.2406\n",
      "> RMSE = 2.1291 | NLL = 2.4601\n",
      "Loading split 2 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0854e+00 RMSE = 8.0041e-01 \n",
      "Samples #    10 : NLL =  2.0771e+00 RMSE = 6.6464e-01 \n",
      "Samples #    15 : NLL =  2.0734e+00 RMSE = 6.0232e-01 \n",
      "Samples #    20 : NLL =  2.0704e+00 RMSE = 5.6267e-01 \n",
      "Samples #    25 : NLL =  2.0672e+00 RMSE = 5.3086e-01 \n",
      "Samples #    30 : NLL =  2.0656e+00 RMSE = 5.1227e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0708e+00 RMSE = 5.1662e-01 \n",
      "Samples #    40 : NLL =  2.0742e+00 RMSE = 5.1758e-01 \n",
      "Samples #    45 : NLL =  2.0746e+00 RMSE = 5.0692e-01 \n",
      "Samples #    50 : NLL =  2.0752e+00 RMSE = 5.0058e-01 \n",
      "Samples #    55 : NLL =  2.0760e+00 RMSE = 4.9314e-01 \n",
      "Samples #    60 : NLL =  2.0755e+00 RMSE = 4.8965e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.0779e+00 RMSE = 4.9359e-01 \n",
      "Samples #    70 : NLL =  2.0793e+00 RMSE = 4.9411e-01 \n",
      "Samples #    75 : NLL =  2.0799e+00 RMSE = 4.9587e-01 \n",
      "Samples #    80 : NLL =  2.0799e+00 RMSE = 4.9171e-01 \n",
      "Samples #    85 : NLL =  2.0796e+00 RMSE = 4.8986e-01 \n",
      "Samples #    90 : NLL =  2.0793e+00 RMSE = 4.8832e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.0807e+00 RMSE = 4.9167e-01 \n",
      "Samples #   100 : NLL =  2.0815e+00 RMSE = 4.9406e-01 \n",
      "Samples #   105 : NLL =  2.0822e+00 RMSE = 4.9244e-01 \n",
      "Samples #   110 : NLL =  2.0820e+00 RMSE = 4.8429e-01 \n",
      "Samples #   115 : NLL =  2.0814e+00 RMSE = 4.7863e-01 \n",
      "Samples #   120 : NLL =  2.0810e+00 RMSE = 4.7339e-01 \n",
      "R-hat: mean 1.2433 std 0.2115\n",
      "> RMSE = 2.8836 | NLL = 2.5670\n",
      "Loading split 3 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0764e+00 RMSE = 8.5206e-01 \n",
      "Samples #    10 : NLL =  2.0662e+00 RMSE = 7.0539e-01 \n",
      "Samples #    15 : NLL =  2.0632e+00 RMSE = 6.3912e-01 \n",
      "Samples #    20 : NLL =  2.0599e+00 RMSE = 5.8486e-01 \n",
      "Samples #    25 : NLL =  2.0589e+00 RMSE = 5.6303e-01 \n",
      "Samples #    30 : NLL =  2.0568e+00 RMSE = 5.3663e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0616e+00 RMSE = 5.4443e-01 \n",
      "Samples #    40 : NLL =  2.0640e+00 RMSE = 5.3923e-01 \n",
      "Samples #    45 : NLL =  2.0650e+00 RMSE = 5.3811e-01 \n",
      "Samples #    50 : NLL =  2.0647e+00 RMSE = 5.2882e-01 \n",
      "Samples #    55 : NLL =  2.0648e+00 RMSE = 5.2715e-01 \n",
      "Samples #    60 : NLL =  2.0641e+00 RMSE = 5.2268e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.0659e+00 RMSE = 5.2725e-01 \n",
      "Samples #    70 : NLL =  2.0656e+00 RMSE = 5.2064e-01 \n",
      "Samples #    75 : NLL =  2.0653e+00 RMSE = 5.1520e-01 \n",
      "Samples #    80 : NLL =  2.0648e+00 RMSE = 5.1134e-01 \n",
      "Samples #    85 : NLL =  2.0645e+00 RMSE = 5.0450e-01 \n",
      "Samples #    90 : NLL =  2.0638e+00 RMSE = 4.9934e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.0646e+00 RMSE = 5.0251e-01 \n",
      "Samples #   100 : NLL =  2.0648e+00 RMSE = 5.0001e-01 \n",
      "Samples #   105 : NLL =  2.0648e+00 RMSE = 4.9618e-01 \n",
      "Samples #   110 : NLL =  2.0648e+00 RMSE = 4.9114e-01 \n",
      "Samples #   115 : NLL =  2.0644e+00 RMSE = 4.8610e-01 \n",
      "Samples #   120 : NLL =  2.0644e+00 RMSE = 4.8054e-01 \n",
      "R-hat: mean 1.2348 std 0.2139\n",
      "> RMSE = 2.2905 | NLL = 2.4855\n",
      "Loading split 4 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0598e+00 RMSE = 7.6034e-01 \n",
      "Samples #    10 : NLL =  2.0541e+00 RMSE = 6.2695e-01 \n",
      "Samples #    15 : NLL =  2.0534e+00 RMSE = 5.8366e-01 \n",
      "Samples #    20 : NLL =  2.0509e+00 RMSE = 5.4275e-01 \n",
      "Samples #    25 : NLL =  2.0483e+00 RMSE = 5.0631e-01 \n",
      "Samples #    30 : NLL =  2.0470e+00 RMSE = 4.8283e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0506e+00 RMSE = 4.8717e-01 \n",
      "Samples #    40 : NLL =  2.0523e+00 RMSE = 4.8554e-01 \n",
      "Samples #    45 : NLL =  2.0526e+00 RMSE = 4.7889e-01 \n",
      "Samples #    50 : NLL =  2.0528e+00 RMSE = 4.7016e-01 \n",
      "Samples #    55 : NLL =  2.0528e+00 RMSE = 4.6709e-01 \n",
      "Samples #    60 : NLL =  2.0528e+00 RMSE = 4.6641e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.0548e+00 RMSE = 4.7322e-01 \n",
      "Samples #    70 : NLL =  2.0556e+00 RMSE = 4.7456e-01 \n",
      "Samples #    75 : NLL =  2.0557e+00 RMSE = 4.7247e-01 \n",
      "Samples #    80 : NLL =  2.0560e+00 RMSE = 4.7430e-01 \n",
      "Samples #    85 : NLL =  2.0556e+00 RMSE = 4.7046e-01 \n",
      "Samples #    90 : NLL =  2.0561e+00 RMSE = 4.7004e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.0566e+00 RMSE = 4.7136e-01 \n",
      "Samples #   100 : NLL =  2.0564e+00 RMSE = 4.6835e-01 \n",
      "Samples #   105 : NLL =  2.0558e+00 RMSE = 4.6557e-01 \n",
      "Samples #   110 : NLL =  2.0554e+00 RMSE = 4.6294e-01 \n",
      "Samples #   115 : NLL =  2.0547e+00 RMSE = 4.5801e-01 \n",
      "Samples #   120 : NLL =  2.0542e+00 RMSE = 4.5552e-01 \n",
      "R-hat: mean 1.2100 std 0.2076\n",
      "> RMSE = 2.4651 | NLL = 2.5021\n",
      "Loading split 5 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0407e+00 RMSE = 7.6742e-01 \n",
      "Samples #    10 : NLL =  2.0338e+00 RMSE = 6.2598e-01 \n",
      "Samples #    15 : NLL =  2.0320e+00 RMSE = 5.6978e-01 \n",
      "Samples #    20 : NLL =  2.0280e+00 RMSE = 5.2174e-01 \n",
      "Samples #    25 : NLL =  2.0243e+00 RMSE = 4.8660e-01 \n",
      "Samples #    30 : NLL =  2.0223e+00 RMSE = 4.6087e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0250e+00 RMSE = 4.6343e-01 \n",
      "Samples #    40 : NLL =  2.0263e+00 RMSE = 4.6536e-01 \n",
      "Samples #    45 : NLL =  2.0271e+00 RMSE = 4.5930e-01 \n",
      "Samples #    50 : NLL =  2.0279e+00 RMSE = 4.5998e-01 \n",
      "Samples #    55 : NLL =  2.0281e+00 RMSE = 4.5496e-01 \n",
      "Samples #    60 : NLL =  2.0285e+00 RMSE = 4.5174e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.0306e+00 RMSE = 4.5234e-01 \n",
      "Samples #    70 : NLL =  2.0311e+00 RMSE = 4.4566e-01 \n",
      "Samples #    75 : NLL =  2.0315e+00 RMSE = 4.4280e-01 \n",
      "Samples #    80 : NLL =  2.0316e+00 RMSE = 4.3991e-01 \n",
      "Samples #    85 : NLL =  2.0317e+00 RMSE = 4.3931e-01 \n",
      "Samples #    90 : NLL =  2.0314e+00 RMSE = 4.3811e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.0326e+00 RMSE = 4.4023e-01 \n",
      "Samples #   100 : NLL =  2.0332e+00 RMSE = 4.4259e-01 \n",
      "Samples #   105 : NLL =  2.0340e+00 RMSE = 4.4472e-01 \n",
      "Samples #   110 : NLL =  2.0342e+00 RMSE = 4.4383e-01 \n",
      "Samples #   115 : NLL =  2.0346e+00 RMSE = 4.4396e-01 \n",
      "Samples #   120 : NLL =  2.0346e+00 RMSE = 4.4539e-01 \n",
      "R-hat: mean 1.1926 std 0.1757\n",
      "> RMSE = 3.5966 | NLL = 2.6080\n",
      "Loading split 6 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0708e+00 RMSE = 7.8152e-01 \n",
      "Samples #    10 : NLL =  2.0639e+00 RMSE = 6.1269e-01 \n",
      "Samples #    15 : NLL =  2.0592e+00 RMSE = 5.4166e-01 \n",
      "Samples #    20 : NLL =  2.0563e+00 RMSE = 5.0220e-01 \n",
      "Samples #    25 : NLL =  2.0531e+00 RMSE = 4.6366e-01 \n",
      "Samples #    30 : NLL =  2.0519e+00 RMSE = 4.4531e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0553e+00 RMSE = 4.4264e-01 \n",
      "Samples #    40 : NLL =  2.0564e+00 RMSE = 4.3830e-01 \n",
      "Samples #    45 : NLL =  2.0561e+00 RMSE = 4.3449e-01 \n",
      "Samples #    50 : NLL =  2.0559e+00 RMSE = 4.3113e-01 \n",
      "Samples #    55 : NLL =  2.0557e+00 RMSE = 4.3341e-01 \n",
      "Samples #    60 : NLL =  2.0557e+00 RMSE = 4.3307e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.0577e+00 RMSE = 4.3679e-01 \n",
      "Samples #    70 : NLL =  2.0578e+00 RMSE = 4.3473e-01 \n",
      "Samples #    75 : NLL =  2.0578e+00 RMSE = 4.3211e-01 \n",
      "Samples #    80 : NLL =  2.0577e+00 RMSE = 4.2809e-01 \n",
      "Samples #    85 : NLL =  2.0575e+00 RMSE = 4.2290e-01 \n",
      "Samples #    90 : NLL =  2.0570e+00 RMSE = 4.1809e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.0585e+00 RMSE = 4.2421e-01 \n",
      "Samples #   100 : NLL =  2.0589e+00 RMSE = 4.2542e-01 \n",
      "Samples #   105 : NLL =  2.0596e+00 RMSE = 4.2581e-01 \n",
      "Samples #   110 : NLL =  2.0598e+00 RMSE = 4.2500e-01 \n",
      "Samples #   115 : NLL =  2.0598e+00 RMSE = 4.2475e-01 \n",
      "Samples #   120 : NLL =  2.0598e+00 RMSE = 4.2206e-01 \n",
      "R-hat: mean 1.2461 std 0.1928\n",
      "> RMSE = 1.7148 | NLL = 2.4408\n",
      "Loading split 7 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.1083e+00 RMSE = 9.3585e-01 \n",
      "Samples #    10 : NLL =  2.1029e+00 RMSE = 7.8530e-01 \n",
      "Samples #    15 : NLL =  2.0992e+00 RMSE = 7.0414e-01 \n",
      "Samples #    20 : NLL =  2.0958e+00 RMSE = 6.4650e-01 \n",
      "Samples #    25 : NLL =  2.0932e+00 RMSE = 6.1179e-01 \n",
      "Samples #    30 : NLL =  2.0924e+00 RMSE = 5.8545e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0968e+00 RMSE = 5.8036e-01 \n",
      "Samples #    40 : NLL =  2.0995e+00 RMSE = 5.7553e-01 \n",
      "Samples #    45 : NLL =  2.1013e+00 RMSE = 5.7909e-01 \n",
      "Samples #    50 : NLL =  2.1028e+00 RMSE = 5.6852e-01 \n",
      "Samples #    55 : NLL =  2.1043e+00 RMSE = 5.6514e-01 \n",
      "Samples #    60 : NLL =  2.1047e+00 RMSE = 5.6549e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.1062e+00 RMSE = 5.6646e-01 \n",
      "Samples #    70 : NLL =  2.1069e+00 RMSE = 5.6801e-01 \n",
      "Samples #    75 : NLL =  2.1070e+00 RMSE = 5.6889e-01 \n",
      "Samples #    80 : NLL =  2.1064e+00 RMSE = 5.6598e-01 \n",
      "Samples #    85 : NLL =  2.1056e+00 RMSE = 5.6309e-01 \n",
      "Samples #    90 : NLL =  2.1048e+00 RMSE = 5.5818e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.1055e+00 RMSE = 5.6000e-01 \n",
      "Samples #   100 : NLL =  2.1064e+00 RMSE = 5.5951e-01 \n",
      "Samples #   105 : NLL =  2.1065e+00 RMSE = 5.5817e-01 \n",
      "Samples #   110 : NLL =  2.1063e+00 RMSE = 5.5564e-01 \n",
      "Samples #   115 : NLL =  2.1059e+00 RMSE = 5.5301e-01 \n",
      "Samples #   120 : NLL =  2.1053e+00 RMSE = 5.4831e-01 \n",
      "R-hat: mean 1.2275 std 0.2200\n",
      "> RMSE = 1.9314 | NLL = 2.4395\n",
      "Loading split 8 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0740e+00 RMSE = 7.4416e-01 \n",
      "Samples #    10 : NLL =  2.0727e+00 RMSE = 6.5414e-01 \n",
      "Samples #    15 : NLL =  2.0702e+00 RMSE = 6.0347e-01 \n",
      "Samples #    20 : NLL =  2.0726e+00 RMSE = 5.5813e-01 \n",
      "Samples #    25 : NLL =  2.0742e+00 RMSE = 5.4956e-01 \n",
      "Samples #    30 : NLL =  2.0714e+00 RMSE = 5.2611e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0738e+00 RMSE = 5.2666e-01 \n",
      "Samples #    40 : NLL =  2.0741e+00 RMSE = 5.2175e-01 \n",
      "Samples #    45 : NLL =  2.0744e+00 RMSE = 5.1877e-01 \n",
      "Samples #    50 : NLL =  2.0739e+00 RMSE = 5.0921e-01 \n",
      "Samples #    55 : NLL =  2.0732e+00 RMSE = 4.9753e-01 \n",
      "Samples #    60 : NLL =  2.0727e+00 RMSE = 4.8877e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.0732e+00 RMSE = 4.9028e-01 \n",
      "Samples #    70 : NLL =  2.0738e+00 RMSE = 4.8974e-01 \n",
      "Samples #    75 : NLL =  2.0735e+00 RMSE = 4.8370e-01 \n",
      "Samples #    80 : NLL =  2.0733e+00 RMSE = 4.8002e-01 \n",
      "Samples #    85 : NLL =  2.0730e+00 RMSE = 4.7863e-01 \n",
      "Samples #    90 : NLL =  2.0727e+00 RMSE = 4.7215e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.0741e+00 RMSE = 4.7261e-01 \n",
      "Samples #   100 : NLL =  2.0747e+00 RMSE = 4.7323e-01 \n",
      "Samples #   105 : NLL =  2.0746e+00 RMSE = 4.7195e-01 \n",
      "Samples #   110 : NLL =  2.0744e+00 RMSE = 4.7122e-01 \n",
      "Samples #   115 : NLL =  2.0744e+00 RMSE = 4.6773e-01 \n",
      "Samples #   120 : NLL =  2.0740e+00 RMSE = 4.6693e-01 \n",
      "R-hat: mean 1.1838 std 0.1327\n",
      "> RMSE = 3.4465 | NLL = 2.5288\n",
      "Loading split 9 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0687e+00 RMSE = 8.0214e-01 \n",
      "Samples #    10 : NLL =  2.0629e+00 RMSE = 6.8158e-01 \n",
      "Samples #    15 : NLL =  2.0582e+00 RMSE = 5.8937e-01 \n",
      "Samples #    20 : NLL =  2.0569e+00 RMSE = 5.5172e-01 \n",
      "Samples #    25 : NLL =  2.0553e+00 RMSE = 5.1261e-01 \n",
      "Samples #    30 : NLL =  2.0529e+00 RMSE = 4.8253e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0552e+00 RMSE = 4.7386e-01 \n",
      "Samples #    40 : NLL =  2.0561e+00 RMSE = 4.6584e-01 \n",
      "Samples #    45 : NLL =  2.0569e+00 RMSE = 4.6285e-01 \n",
      "Samples #    50 : NLL =  2.0566e+00 RMSE = 4.5950e-01 \n",
      "Samples #    55 : NLL =  2.0564e+00 RMSE = 4.5707e-01 \n",
      "Samples #    60 : NLL =  2.0563e+00 RMSE = 4.5238e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.0578e+00 RMSE = 4.5419e-01 \n",
      "Samples #    70 : NLL =  2.0581e+00 RMSE = 4.4812e-01 \n",
      "Samples #    75 : NLL =  2.0579e+00 RMSE = 4.4686e-01 \n",
      "Samples #    80 : NLL =  2.0573e+00 RMSE = 4.4177e-01 \n",
      "Samples #    85 : NLL =  2.0572e+00 RMSE = 4.3922e-01 \n",
      "Samples #    90 : NLL =  2.0570e+00 RMSE = 4.3563e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.0577e+00 RMSE = 4.3571e-01 \n",
      "Samples #   100 : NLL =  2.0583e+00 RMSE = 4.4015e-01 \n",
      "Samples #   105 : NLL =  2.0586e+00 RMSE = 4.4090e-01 \n",
      "Samples #   110 : NLL =  2.0588e+00 RMSE = 4.3879e-01 \n",
      "Samples #   115 : NLL =  2.0590e+00 RMSE = 4.3729e-01 \n",
      "Samples #   120 : NLL =  2.0592e+00 RMSE = 4.3600e-01 \n",
      "R-hat: mean 1.2571 std 0.2049\n",
      "> RMSE = 3.6169 | NLL = 2.6642\n",
      "Loading split 10 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0678e+00 RMSE = 8.1951e-01 \n",
      "Samples #    10 : NLL =  2.0605e+00 RMSE = 6.7817e-01 \n",
      "Samples #    15 : NLL =  2.0536e+00 RMSE = 5.8176e-01 \n",
      "Samples #    20 : NLL =  2.0509e+00 RMSE = 5.3014e-01 \n",
      "Samples #    25 : NLL =  2.0498e+00 RMSE = 5.0295e-01 \n",
      "Samples #    30 : NLL =  2.0492e+00 RMSE = 4.8731e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0524e+00 RMSE = 4.9411e-01 \n",
      "Samples #    40 : NLL =  2.0542e+00 RMSE = 4.9388e-01 \n",
      "Samples #    45 : NLL =  2.0556e+00 RMSE = 4.8855e-01 \n",
      "Samples #    50 : NLL =  2.0561e+00 RMSE = 4.8762e-01 \n",
      "Samples #    55 : NLL =  2.0569e+00 RMSE = 4.9619e-01 \n",
      "Samples #    60 : NLL =  2.0571e+00 RMSE = 4.9466e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.0590e+00 RMSE = 4.9832e-01 \n",
      "Samples #    70 : NLL =  2.0594e+00 RMSE = 4.9948e-01 \n",
      "Samples #    75 : NLL =  2.0601e+00 RMSE = 5.0006e-01 \n",
      "Samples #    80 : NLL =  2.0604e+00 RMSE = 4.9555e-01 \n",
      "Samples #    85 : NLL =  2.0606e+00 RMSE = 4.8969e-01 \n",
      "Samples #    90 : NLL =  2.0609e+00 RMSE = 4.8381e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.0618e+00 RMSE = 4.8795e-01 \n",
      "Samples #   100 : NLL =  2.0622e+00 RMSE = 4.8917e-01 \n",
      "Samples #   105 : NLL =  2.0622e+00 RMSE = 4.8896e-01 \n",
      "Samples #   110 : NLL =  2.0616e+00 RMSE = 4.8370e-01 \n",
      "Samples #   115 : NLL =  2.0613e+00 RMSE = 4.7950e-01 \n",
      "Samples #   120 : NLL =  2.0611e+00 RMSE = 4.7534e-01 \n",
      "R-hat: mean 1.2400 std 0.2026\n",
      "> RMSE = 4.0951 | NLL = 2.6458\n"
     ]
    }
   ],
   "source": [
    "results = {\"rmse\": [], \"nll\": []}\n",
    "\n",
    "for split_id in range(n_splits):\n",
    "    print(\"Loading split {} of {} dataset\".format(split_id+1, dataset))\n",
    "    saved_dir = os.path.join(out_dir, str(split_id))\n",
    "    \n",
    "    # Load the dataset\n",
    "    X_train, y_train, X_test, y_test = util.load_uci_data(\n",
    "            data_dir, split_id, dataset)\n",
    "    input_dim, output_dim = int(X_train.shape[-1]), 1\n",
    "\n",
    "    # Initialize the neural network and likelihood modules\n",
    "    weight_mask, bias_mask = masks_list[split_id]\n",
    "    net = MLPMasked(input_dim, output_dim, [n_units] * n_hidden, activation_fn, weight_mask, bias_mask)\n",
    "    likelihood = LikGaussian(noise_var)\n",
    "    \n",
    "    # Load the optimized prior\n",
    "    ckpt_path = os.path.join(out_dir, str(split_id), \"ckpts\", \"sparse-it-{}.ckpt\".format(num_iters_sd))\n",
    "    prior = OptimGaussianPrior(ckpt_path)\n",
    "    \n",
    "    # Initialize bayesian neural network with SGHMC sampler\n",
    "    saved_dir = os.path.join(out_dir, str(split_id))\n",
    "    bayes_net = RegressionNetMasked(net, likelihood, prior, saved_dir, n_gpu=0)\n",
    "    \n",
    "    # Start sampling\n",
    "    bayes_net.sample_multi_chains(X_train, y_train, **sampling_configs)\n",
    "    pred_mean, pred_var, preds, raw_preds = bayes_net.predict(X_test, True, True)\n",
    "    r_hat = compute_rhat_regression(raw_preds, sampling_configs[\"num_chains\"])\n",
    "    print(\"R-hat: mean {:.4f} std {:.4f}\".format(float(r_hat.mean()), float(r_hat.std())))\n",
    "\n",
    "    rmse = uncertainty_metrics.rmse(pred_mean, y_test)\n",
    "    nll = uncertainty_metrics.gaussian_nll(y_test, pred_mean, pred_var)\n",
    "    print(\"> RMSE = {:.4f} | NLL = {:.4f}\".format(rmse, nll))\n",
    "    results['rmse'].append(rmse)\n",
    "    results['nll'].append(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_no = pd.DataFrame(results)\n",
    "result_df_no.to_csv(os.path.join(out_dir, \"optim_results.csv\"), sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_configs = {\n",
    "    \"batch_size\": 32,                 # Mini-batch size\n",
    "    \"num_samples\": 40,                # Total number of samples for each chain \n",
    "    \"n_discarded\": 10,                # Number of the first samples to be discared for each chain\n",
    "    \"num_burn_in_steps\": 2000,         # Number of burn-in steps\n",
    "    \"keep_every\": 2000,                # Thinning interval\n",
    "    \"lr\": 1e-2,                       # Step size\n",
    "    \"num_chains\": 4,                  # Number of chains\n",
    "    \"mdecay\": 1e-2,                   # Momentum coefficient\n",
    "    \"print_every_n_samples\": 5,\n",
    "    \"prevent_overfitting\" : \"Dropout\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading split 1 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0736e+00 RMSE = 7.2658e-01 \n",
      "Samples #    10 : NLL =  2.0722e+00 RMSE = 6.1025e-01 \n",
      "Samples #    15 : NLL =  2.0697e+00 RMSE = 5.6501e-01 \n",
      "Samples #    20 : NLL =  2.0682e+00 RMSE = 5.1987e-01 \n",
      "Samples #    25 : NLL =  2.0658e+00 RMSE = 4.8594e-01 \n",
      "Samples #    30 : NLL =  2.0635e+00 RMSE = 4.6337e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0663e+00 RMSE = 4.6111e-01 \n",
      "Samples #    40 : NLL =  2.0689e+00 RMSE = 4.5889e-01 \n",
      "Samples #    45 : NLL =  2.0699e+00 RMSE = 4.5372e-01 \n",
      "Samples #    50 : NLL =  2.0702e+00 RMSE = 4.4928e-01 \n",
      "Samples #    55 : NLL =  2.0701e+00 RMSE = 4.4529e-01 \n",
      "Samples #    60 : NLL =  2.0703e+00 RMSE = 4.3806e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.0723e+00 RMSE = 4.4186e-01 \n",
      "Samples #    70 : NLL =  2.0737e+00 RMSE = 4.4463e-01 \n",
      "Samples #    75 : NLL =  2.0741e+00 RMSE = 4.4604e-01 \n",
      "Samples #    80 : NLL =  2.0746e+00 RMSE = 4.4779e-01 \n",
      "Samples #    85 : NLL =  2.0745e+00 RMSE = 4.4810e-01 \n",
      "Samples #    90 : NLL =  2.0746e+00 RMSE = 4.4477e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.0760e+00 RMSE = 4.5003e-01 \n",
      "Samples #   100 : NLL =  2.0767e+00 RMSE = 4.5572e-01 \n",
      "Samples #   105 : NLL =  2.0769e+00 RMSE = 4.5554e-01 \n",
      "Samples #   110 : NLL =  2.0768e+00 RMSE = 4.5458e-01 \n",
      "Samples #   115 : NLL =  2.0769e+00 RMSE = 4.5387e-01 \n",
      "Samples #   120 : NLL =  2.0765e+00 RMSE = 4.5230e-01 \n",
      "R-hat: mean 1.3720 std 0.2972\n",
      "> RMSE = 2.5289 | NLL = 2.4987\n",
      "Loading split 2 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0892e+00 RMSE = 8.4667e-01 \n",
      "Samples #    10 : NLL =  2.0854e+00 RMSE = 7.1627e-01 \n",
      "Samples #    15 : NLL =  2.0819e+00 RMSE = 6.6076e-01 \n",
      "Samples #    20 : NLL =  2.0793e+00 RMSE = 6.1086e-01 \n",
      "Samples #    25 : NLL =  2.0764e+00 RMSE = 5.8326e-01 \n",
      "Samples #    30 : NLL =  2.0741e+00 RMSE = 5.5992e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0768e+00 RMSE = 5.5660e-01 \n",
      "Samples #    40 : NLL =  2.0786e+00 RMSE = 5.5814e-01 \n",
      "Samples #    45 : NLL =  2.0792e+00 RMSE = 5.5211e-01 \n",
      "Samples #    50 : NLL =  2.0794e+00 RMSE = 5.4471e-01 \n",
      "Samples #    55 : NLL =  2.0787e+00 RMSE = 5.3720e-01 \n",
      "Samples #    60 : NLL =  2.0782e+00 RMSE = 5.2435e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.0787e+00 RMSE = 5.2329e-01 \n",
      "Samples #    70 : NLL =  2.0780e+00 RMSE = 5.1759e-01 \n",
      "Samples #    75 : NLL =  2.0780e+00 RMSE = 5.1277e-01 \n",
      "Samples #    80 : NLL =  2.0778e+00 RMSE = 5.0768e-01 \n",
      "Samples #    85 : NLL =  2.0772e+00 RMSE = 4.9890e-01 \n",
      "Samples #    90 : NLL =  2.0766e+00 RMSE = 4.9346e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.0774e+00 RMSE = 4.9454e-01 \n",
      "Samples #   100 : NLL =  2.0779e+00 RMSE = 4.9505e-01 \n",
      "Samples #   105 : NLL =  2.0783e+00 RMSE = 4.9582e-01 \n",
      "Samples #   110 : NLL =  2.0782e+00 RMSE = 4.9406e-01 \n",
      "Samples #   115 : NLL =  2.0778e+00 RMSE = 4.9286e-01 \n",
      "Samples #   120 : NLL =  2.0777e+00 RMSE = 4.9032e-01 \n",
      "R-hat: mean 1.2868 std 0.2632\n",
      "> RMSE = 2.4996 | NLL = 2.5058\n",
      "Loading split 3 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0717e+00 RMSE = 7.9561e-01 \n",
      "Samples #    10 : NLL =  2.0710e+00 RMSE = 6.8165e-01 \n",
      "Samples #    15 : NLL =  2.0701e+00 RMSE = 6.3657e-01 \n",
      "Samples #    20 : NLL =  2.0669e+00 RMSE = 5.8654e-01 \n",
      "Samples #    25 : NLL =  2.0646e+00 RMSE = 5.4927e-01 \n",
      "Samples #    30 : NLL =  2.0626e+00 RMSE = 5.2867e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0636e+00 RMSE = 5.1248e-01 \n",
      "Samples #    40 : NLL =  2.0634e+00 RMSE = 5.0179e-01 \n",
      "Samples #    45 : NLL =  2.0636e+00 RMSE = 4.9595e-01 \n",
      "Samples #    50 : NLL =  2.0636e+00 RMSE = 4.9024e-01 \n",
      "Samples #    55 : NLL =  2.0631e+00 RMSE = 4.8593e-01 \n",
      "Samples #    60 : NLL =  2.0631e+00 RMSE = 4.8030e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.0642e+00 RMSE = 4.8069e-01 \n",
      "Samples #    70 : NLL =  2.0640e+00 RMSE = 4.8045e-01 \n",
      "Samples #    75 : NLL =  2.0635e+00 RMSE = 4.7696e-01 \n",
      "Samples #    80 : NLL =  2.0634e+00 RMSE = 4.7046e-01 \n",
      "Samples #    85 : NLL =  2.0630e+00 RMSE = 4.6797e-01 \n",
      "Samples #    90 : NLL =  2.0623e+00 RMSE = 4.6137e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.0630e+00 RMSE = 4.6377e-01 \n",
      "Samples #   100 : NLL =  2.0636e+00 RMSE = 4.6301e-01 \n",
      "Samples #   105 : NLL =  2.0639e+00 RMSE = 4.6053e-01 \n",
      "Samples #   110 : NLL =  2.0636e+00 RMSE = 4.5736e-01 \n",
      "Samples #   115 : NLL =  2.0633e+00 RMSE = 4.5377e-01 \n",
      "Samples #   120 : NLL =  2.0628e+00 RMSE = 4.4954e-01 \n",
      "R-hat: mean 1.2425 std 0.2539\n",
      "> RMSE = 2.2356 | NLL = 2.4687\n",
      "Loading split 4 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0605e+00 RMSE = 7.9275e-01 \n",
      "Samples #    10 : NLL =  2.0568e+00 RMSE = 6.3975e-01 \n",
      "Samples #    15 : NLL =  2.0538e+00 RMSE = 5.7056e-01 \n",
      "Samples #    20 : NLL =  2.0503e+00 RMSE = 5.1795e-01 \n",
      "Samples #    25 : NLL =  2.0483e+00 RMSE = 4.9931e-01 \n",
      "Samples #    30 : NLL =  2.0448e+00 RMSE = 4.6792e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0492e+00 RMSE = 4.6341e-01 \n",
      "Samples #    40 : NLL =  2.0501e+00 RMSE = 4.6402e-01 \n",
      "Samples #    45 : NLL =  2.0501e+00 RMSE = 4.5974e-01 \n",
      "Samples #    50 : NLL =  2.0497e+00 RMSE = 4.5716e-01 \n",
      "Samples #    55 : NLL =  2.0487e+00 RMSE = 4.5044e-01 \n",
      "Samples #    60 : NLL =  2.0479e+00 RMSE = 4.4711e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.0501e+00 RMSE = 4.5054e-01 \n",
      "Samples #    70 : NLL =  2.0511e+00 RMSE = 4.5038e-01 \n",
      "Samples #    75 : NLL =  2.0518e+00 RMSE = 4.4666e-01 \n",
      "Samples #    80 : NLL =  2.0518e+00 RMSE = 4.4061e-01 \n",
      "Samples #    85 : NLL =  2.0514e+00 RMSE = 4.3499e-01 \n",
      "Samples #    90 : NLL =  2.0508e+00 RMSE = 4.2452e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.0511e+00 RMSE = 4.2722e-01 \n",
      "Samples #   100 : NLL =  2.0515e+00 RMSE = 4.2941e-01 \n",
      "Samples #   105 : NLL =  2.0518e+00 RMSE = 4.3059e-01 \n",
      "Samples #   110 : NLL =  2.0520e+00 RMSE = 4.3048e-01 \n",
      "Samples #   115 : NLL =  2.0522e+00 RMSE = 4.3116e-01 \n",
      "Samples #   120 : NLL =  2.0521e+00 RMSE = 4.3035e-01 \n",
      "R-hat: mean 1.2462 std 0.1880\n",
      "> RMSE = 2.7024 | NLL = 2.5153\n",
      "Loading split 5 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0414e+00 RMSE = 7.9542e-01 \n",
      "Samples #    10 : NLL =  2.0387e+00 RMSE = 6.9265e-01 \n",
      "Samples #    15 : NLL =  2.0345e+00 RMSE = 6.1806e-01 \n",
      "Samples #    20 : NLL =  2.0298e+00 RMSE = 5.6430e-01 \n",
      "Samples #    25 : NLL =  2.0278e+00 RMSE = 5.4046e-01 \n",
      "Samples #    30 : NLL =  2.0257e+00 RMSE = 5.1225e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0295e+00 RMSE = 5.0643e-01 \n",
      "Samples #    40 : NLL =  2.0315e+00 RMSE = 5.0016e-01 \n",
      "Samples #    45 : NLL =  2.0323e+00 RMSE = 4.9111e-01 \n",
      "Samples #    50 : NLL =  2.0342e+00 RMSE = 4.9209e-01 \n",
      "Samples #    55 : NLL =  2.0343e+00 RMSE = 4.8558e-01 \n",
      "Samples #    60 : NLL =  2.0344e+00 RMSE = 4.7488e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.0366e+00 RMSE = 4.7956e-01 \n",
      "Samples #    70 : NLL =  2.0372e+00 RMSE = 4.7669e-01 \n",
      "Samples #    75 : NLL =  2.0377e+00 RMSE = 4.7292e-01 \n",
      "Samples #    80 : NLL =  2.0378e+00 RMSE = 4.6786e-01 \n",
      "Samples #    85 : NLL =  2.0376e+00 RMSE = 4.6453e-01 \n",
      "Samples #    90 : NLL =  2.0375e+00 RMSE = 4.6349e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.0387e+00 RMSE = 4.6599e-01 \n",
      "Samples #   100 : NLL =  2.0397e+00 RMSE = 4.6939e-01 \n",
      "Samples #   105 : NLL =  2.0403e+00 RMSE = 4.7119e-01 \n",
      "Samples #   110 : NLL =  2.0405e+00 RMSE = 4.7047e-01 \n",
      "Samples #   115 : NLL =  2.0404e+00 RMSE = 4.6910e-01 \n",
      "Samples #   120 : NLL =  2.0404e+00 RMSE = 4.6817e-01 \n",
      "R-hat: mean 1.1897 std 0.1733\n",
      "> RMSE = 4.0420 | NLL = 2.6909\n",
      "Loading split 6 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0687e+00 RMSE = 8.2762e-01 \n",
      "Samples #    10 : NLL =  2.0602e+00 RMSE = 6.7660e-01 \n",
      "Samples #    15 : NLL =  2.0551e+00 RMSE = 6.0338e-01 \n",
      "Samples #    20 : NLL =  2.0538e+00 RMSE = 5.4684e-01 \n",
      "Samples #    25 : NLL =  2.0518e+00 RMSE = 5.0800e-01 \n",
      "Samples #    30 : NLL =  2.0495e+00 RMSE = 4.8234e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0538e+00 RMSE = 4.8431e-01 \n",
      "Samples #    40 : NLL =  2.0570e+00 RMSE = 4.8629e-01 \n",
      "Samples #    45 : NLL =  2.0582e+00 RMSE = 4.8352e-01 \n",
      "Samples #    50 : NLL =  2.0589e+00 RMSE = 4.8197e-01 \n",
      "Samples #    55 : NLL =  2.0584e+00 RMSE = 4.7569e-01 \n",
      "Samples #    60 : NLL =  2.0576e+00 RMSE = 4.6540e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.0608e+00 RMSE = 4.7062e-01 \n",
      "Samples #    70 : NLL =  2.0618e+00 RMSE = 4.7147e-01 \n",
      "Samples #    75 : NLL =  2.0618e+00 RMSE = 4.6965e-01 \n",
      "Samples #    80 : NLL =  2.0617e+00 RMSE = 4.6455e-01 \n",
      "Samples #    85 : NLL =  2.0614e+00 RMSE = 4.5693e-01 \n",
      "Samples #    90 : NLL =  2.0610e+00 RMSE = 4.5005e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.0619e+00 RMSE = 4.5219e-01 \n",
      "Samples #   100 : NLL =  2.0624e+00 RMSE = 4.4944e-01 \n",
      "Samples #   105 : NLL =  2.0622e+00 RMSE = 4.4830e-01 \n",
      "Samples #   110 : NLL =  2.0618e+00 RMSE = 4.4385e-01 \n",
      "Samples #   115 : NLL =  2.0616e+00 RMSE = 4.3929e-01 \n",
      "Samples #   120 : NLL =  2.0614e+00 RMSE = 4.3681e-01 \n",
      "R-hat: mean 1.2094 std 0.1854\n",
      "> RMSE = 1.6128 | NLL = 2.4134\n",
      "Loading split 7 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0986e+00 RMSE = 8.3960e-01 \n",
      "Samples #    10 : NLL =  2.0965e+00 RMSE = 7.3356e-01 \n",
      "Samples #    15 : NLL =  2.0954e+00 RMSE = 6.8055e-01 \n",
      "Samples #    20 : NLL =  2.0926e+00 RMSE = 6.2646e-01 \n",
      "Samples #    25 : NLL =  2.0897e+00 RMSE = 5.8741e-01 \n",
      "Samples #    30 : NLL =  2.0872e+00 RMSE = 5.6416e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0933e+00 RMSE = 5.6921e-01 \n",
      "Samples #    40 : NLL =  2.0958e+00 RMSE = 5.6564e-01 \n",
      "Samples #    45 : NLL =  2.0967e+00 RMSE = 5.5873e-01 \n",
      "Samples #    50 : NLL =  2.0974e+00 RMSE = 5.5277e-01 \n",
      "Samples #    55 : NLL =  2.0971e+00 RMSE = 5.4326e-01 \n",
      "Samples #    60 : NLL =  2.0964e+00 RMSE = 5.3751e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.0970e+00 RMSE = 5.3723e-01 \n",
      "Samples #    70 : NLL =  2.0975e+00 RMSE = 5.3216e-01 \n",
      "Samples #    75 : NLL =  2.0980e+00 RMSE = 5.3384e-01 \n",
      "Samples #    80 : NLL =  2.0983e+00 RMSE = 5.3619e-01 \n",
      "Samples #    85 : NLL =  2.0980e+00 RMSE = 5.3255e-01 \n",
      "Samples #    90 : NLL =  2.0971e+00 RMSE = 5.2606e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.0980e+00 RMSE = 5.2993e-01 \n",
      "Samples #   100 : NLL =  2.0979e+00 RMSE = 5.2803e-01 \n",
      "Samples #   105 : NLL =  2.0975e+00 RMSE = 5.2210e-01 \n",
      "Samples #   110 : NLL =  2.0970e+00 RMSE = 5.1742e-01 \n",
      "Samples #   115 : NLL =  2.0964e+00 RMSE = 5.1506e-01 \n",
      "Samples #   120 : NLL =  2.0955e+00 RMSE = 5.0877e-01 \n",
      "R-hat: mean 1.3017 std 0.3153\n",
      "> RMSE = 2.0553 | NLL = 2.4397\n",
      "Loading split 8 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0875e+00 RMSE = 8.1394e-01 \n",
      "Samples #    10 : NLL =  2.0845e+00 RMSE = 6.9125e-01 \n",
      "Samples #    15 : NLL =  2.0809e+00 RMSE = 6.2733e-01 \n",
      "Samples #    20 : NLL =  2.0766e+00 RMSE = 5.8673e-01 \n",
      "Samples #    25 : NLL =  2.0733e+00 RMSE = 5.5286e-01 \n",
      "Samples #    30 : NLL =  2.0721e+00 RMSE = 5.2887e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0746e+00 RMSE = 5.2637e-01 \n",
      "Samples #    40 : NLL =  2.0758e+00 RMSE = 5.2572e-01 \n",
      "Samples #    45 : NLL =  2.0748e+00 RMSE = 5.1434e-01 \n",
      "Samples #    50 : NLL =  2.0739e+00 RMSE = 5.0561e-01 \n",
      "Samples #    55 : NLL =  2.0732e+00 RMSE = 4.9997e-01 \n",
      "Samples #    60 : NLL =  2.0731e+00 RMSE = 4.9247e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.0743e+00 RMSE = 4.9582e-01 \n",
      "Samples #    70 : NLL =  2.0748e+00 RMSE = 4.9252e-01 \n",
      "Samples #    75 : NLL =  2.0748e+00 RMSE = 4.9081e-01 \n",
      "Samples #    80 : NLL =  2.0747e+00 RMSE = 4.8748e-01 \n",
      "Samples #    85 : NLL =  2.0744e+00 RMSE = 4.8097e-01 \n",
      "Samples #    90 : NLL =  2.0739e+00 RMSE = 4.7599e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.0747e+00 RMSE = 4.7583e-01 \n",
      "Samples #   100 : NLL =  2.0751e+00 RMSE = 4.7713e-01 \n",
      "Samples #   105 : NLL =  2.0757e+00 RMSE = 4.7922e-01 \n",
      "Samples #   110 : NLL =  2.0760e+00 RMSE = 4.7624e-01 \n",
      "Samples #   115 : NLL =  2.0762e+00 RMSE = 4.7384e-01 \n",
      "Samples #   120 : NLL =  2.0760e+00 RMSE = 4.7059e-01 \n",
      "R-hat: mean 1.2397 std 0.2451\n",
      "> RMSE = 3.2557 | NLL = 2.5536\n",
      "Loading split 9 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0649e+00 RMSE = 7.7062e-01 \n",
      "Samples #    10 : NLL =  2.0602e+00 RMSE = 6.5579e-01 \n",
      "Samples #    15 : NLL =  2.0569e+00 RMSE = 5.8831e-01 \n",
      "Samples #    20 : NLL =  2.0552e+00 RMSE = 5.3996e-01 \n",
      "Samples #    25 : NLL =  2.0533e+00 RMSE = 5.0325e-01 \n",
      "Samples #    30 : NLL =  2.0512e+00 RMSE = 4.7976e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0536e+00 RMSE = 4.7063e-01 \n",
      "Samples #    40 : NLL =  2.0536e+00 RMSE = 4.6714e-01 \n",
      "Samples #    45 : NLL =  2.0538e+00 RMSE = 4.5664e-01 \n",
      "Samples #    50 : NLL =  2.0538e+00 RMSE = 4.5211e-01 \n",
      "Samples #    55 : NLL =  2.0532e+00 RMSE = 4.4664e-01 \n",
      "Samples #    60 : NLL =  2.0524e+00 RMSE = 4.4323e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.0541e+00 RMSE = 4.4804e-01 \n",
      "Samples #    70 : NLL =  2.0549e+00 RMSE = 4.4735e-01 \n",
      "Samples #    75 : NLL =  2.0555e+00 RMSE = 4.4893e-01 \n",
      "Samples #    80 : NLL =  2.0554e+00 RMSE = 4.4522e-01 \n",
      "Samples #    85 : NLL =  2.0552e+00 RMSE = 4.4385e-01 \n",
      "Samples #    90 : NLL =  2.0550e+00 RMSE = 4.3974e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.0561e+00 RMSE = 4.4400e-01 \n",
      "Samples #   100 : NLL =  2.0567e+00 RMSE = 4.4552e-01 \n",
      "Samples #   105 : NLL =  2.0569e+00 RMSE = 4.4682e-01 \n",
      "Samples #   110 : NLL =  2.0567e+00 RMSE = 4.4379e-01 \n",
      "Samples #   115 : NLL =  2.0565e+00 RMSE = 4.3852e-01 \n",
      "Samples #   120 : NLL =  2.0563e+00 RMSE = 4.3664e-01 \n",
      "R-hat: mean 1.2608 std 0.2358\n",
      "> RMSE = 3.4392 | NLL = 2.6620\n",
      "Loading split 10 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0724e+00 RMSE = 8.7639e-01 \n",
      "Samples #    10 : NLL =  2.0679e+00 RMSE = 7.3883e-01 \n",
      "Samples #    15 : NLL =  2.0607e+00 RMSE = 6.4715e-01 \n",
      "Samples #    20 : NLL =  2.0571e+00 RMSE = 5.9642e-01 \n",
      "Samples #    25 : NLL =  2.0545e+00 RMSE = 5.5564e-01 \n",
      "Samples #    30 : NLL =  2.0520e+00 RMSE = 5.2719e-01 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.0572e+00 RMSE = 5.4149e-01 \n",
      "Samples #    40 : NLL =  2.0605e+00 RMSE = 5.4136e-01 \n",
      "Samples #    45 : NLL =  2.0614e+00 RMSE = 5.4242e-01 \n",
      "Samples #    50 : NLL =  2.0610e+00 RMSE = 5.4235e-01 \n",
      "Samples #    55 : NLL =  2.0602e+00 RMSE = 5.3458e-01 \n",
      "Samples #    60 : NLL =  2.0593e+00 RMSE = 5.2293e-01 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.0603e+00 RMSE = 5.2615e-01 \n",
      "Samples #    70 : NLL =  2.0619e+00 RMSE = 5.2562e-01 \n",
      "Samples #    75 : NLL =  2.0622e+00 RMSE = 5.2689e-01 \n",
      "Samples #    80 : NLL =  2.0621e+00 RMSE = 5.2111e-01 \n",
      "Samples #    85 : NLL =  2.0620e+00 RMSE = 5.1960e-01 \n",
      "Samples #    90 : NLL =  2.0625e+00 RMSE = 5.1724e-01 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.0628e+00 RMSE = 5.1908e-01 \n",
      "Samples #   100 : NLL =  2.0628e+00 RMSE = 5.2069e-01 \n",
      "Samples #   105 : NLL =  2.0627e+00 RMSE = 5.1884e-01 \n",
      "Samples #   110 : NLL =  2.0627e+00 RMSE = 5.1848e-01 \n",
      "Samples #   115 : NLL =  2.0622e+00 RMSE = 5.1362e-01 \n",
      "Samples #   120 : NLL =  2.0614e+00 RMSE = 5.0921e-01 \n",
      "R-hat: mean 1.1801 std 0.1882\n",
      "> RMSE = 4.6082 | NLL = 2.6272\n"
     ]
    }
   ],
   "source": [
    "results = {\"rmse\": [], \"nll\": []}\n",
    "\n",
    "for split_id in range(n_splits):\n",
    "    print(\"Loading split {} of {} dataset\".format(split_id+1, dataset))\n",
    "    saved_dir = os.path.join(out_dir, str(split_id))\n",
    "    \n",
    "    # Load the dataset\n",
    "    X_train, y_train, X_test, y_test = util.load_uci_data(\n",
    "            data_dir, split_id, dataset)\n",
    "    input_dim, output_dim = int(X_train.shape[-1]), 1\n",
    "\n",
    "    # Initialize the neural network and likelihood modules\n",
    "    weight_mask, bias_mask = masks_list[split_id]\n",
    "    net = MLPMasked(input_dim, output_dim, [n_units] * n_hidden, activation_fn, weight_mask, bias_mask)\n",
    "    likelihood = LikGaussian(noise_var)\n",
    "    \n",
    "    # Load the optimized prior\n",
    "    ckpt_path = os.path.join(out_dir, str(split_id), \"ckpts\", \"sparse-it-{}.ckpt\".format(num_iters_sd))\n",
    "    prior = OptimGaussianPrior(ckpt_path)\n",
    "    \n",
    "    # Initialize bayesian neural network with SGHMC sampler\n",
    "    saved_dir = os.path.join(out_dir, str(split_id))\n",
    "    bayes_net = RegressionNetMasked(net, likelihood, prior, saved_dir, n_gpu=0)\n",
    "    \n",
    "    # Start sampling\n",
    "    bayes_net.sample_multi_chains(X_train, y_train, **sampling_configs)\n",
    "    pred_mean, pred_var, preds, raw_preds = bayes_net.predict(X_test, True, True)\n",
    "    r_hat = compute_rhat_regression(raw_preds, sampling_configs[\"num_chains\"])\n",
    "    print(\"R-hat: mean {:.4f} std {:.4f}\".format(float(r_hat.mean()), float(r_hat.std())))\n",
    "\n",
    "    rmse = uncertainty_metrics.rmse(pred_mean, y_test)\n",
    "    nll = uncertainty_metrics.gaussian_nll(y_test, pred_mean, pred_var)\n",
    "    print(\"> RMSE = {:.4f} | NLL = {:.4f}\".format(rmse, nll))\n",
    "    results['rmse'].append(rmse)\n",
    "    results['nll'].append(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_do = pd.DataFrame(results)\n",
    "result_df_do.to_csv(os.path.join(out_dir, \"optim_results_dropout.csv\"), sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping to Prevent Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_configs = {\n",
    "    \"batch_size\": 32,                 # Mini-batch size\n",
    "    \"num_samples\": 40,                # Total number of samples for each chain \n",
    "    \"n_discarded\": 10,                # Number of the first samples to be discared for each chain\n",
    "    \"num_burn_in_steps\": 2000,         # Number of burn-in steps\n",
    "    \"keep_every\": 2000,                # Thinning interval\n",
    "    \"lr\": 1e-2,                       # Step size\n",
    "    \"num_chains\": 4,                  # Number of chains\n",
    "    \"mdecay\": 1e-2,                   # Momentum coefficient\n",
    "    \"print_every_n_samples\": 5,\n",
    "    \"prevent_overfitting\" : \"Early Stopping\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading split 1 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.1676e+00 RMSE = 1.2479e+00 \n",
      "Samples #    10 : NLL =  2.2233e+00 RMSE = 1.3703e+00 \n",
      "Samples #    15 : NLL =  2.2403e+00 RMSE = 1.3806e+00 \n",
      "Samples #    20 : NLL =  2.2425e+00 RMSE = 1.3706e+00 \n",
      "Samples #    25 : NLL =  2.2476e+00 RMSE = 1.3766e+00 \n",
      "Samples #    30 : NLL =  2.2523e+00 RMSE = 1.3742e+00 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.2389e+00 RMSE = 1.2742e+00 \n",
      "Samples #    40 : NLL =  2.2402e+00 RMSE = 1.2552e+00 \n",
      "Samples #    45 : NLL =  2.2420e+00 RMSE = 1.2460e+00 \n",
      "Samples #    50 : NLL =  2.2443e+00 RMSE = 1.2479e+00 \n",
      "Samples #    55 : NLL =  2.2454e+00 RMSE = 1.2443e+00 \n",
      "Samples #    60 : NLL =  2.2469e+00 RMSE = 1.2376e+00 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.2403e+00 RMSE = 1.2044e+00 \n",
      "Samples #    70 : NLL =  2.2422e+00 RMSE = 1.2081e+00 \n",
      "Samples #    75 : NLL =  2.2439e+00 RMSE = 1.2185e+00 \n",
      "Samples #    80 : NLL =  2.2450e+00 RMSE = 1.2192e+00 \n",
      "Samples #    85 : NLL =  2.2465e+00 RMSE = 1.2261e+00 \n",
      "Samples #    90 : NLL =  2.2477e+00 RMSE = 1.2303e+00 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.2426e+00 RMSE = 1.2009e+00 \n",
      "Samples #   100 : NLL =  2.2427e+00 RMSE = 1.1972e+00 \n",
      "Samples #   105 : NLL =  2.2428e+00 RMSE = 1.1949e+00 \n",
      "Samples #   110 : NLL =  2.2437e+00 RMSE = 1.1979e+00 \n",
      "Samples #   115 : NLL =  2.2443e+00 RMSE = 1.1973e+00 \n",
      "Samples #   120 : NLL =  2.2444e+00 RMSE = 1.1980e+00 \n",
      "R-hat: mean 1.1382 std 0.1147\n",
      "> RMSE = 2.5490 | NLL = 2.4962\n",
      "Loading split 2 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.1591e+00 RMSE = 1.1617e+00 \n",
      "Samples #    10 : NLL =  2.2193e+00 RMSE = 1.3328e+00 \n",
      "Samples #    15 : NLL =  2.2351e+00 RMSE = 1.3752e+00 \n",
      "Samples #    20 : NLL =  2.2420e+00 RMSE = 1.3839e+00 \n",
      "Samples #    25 : NLL =  2.2494e+00 RMSE = 1.4029e+00 \n",
      "Samples #    30 : NLL =  2.2542e+00 RMSE = 1.4070e+00 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.2403e+00 RMSE = 1.2976e+00 \n",
      "Samples #    40 : NLL =  2.2424e+00 RMSE = 1.2694e+00 \n",
      "Samples #    45 : NLL =  2.2432e+00 RMSE = 1.2552e+00 \n",
      "Samples #    50 : NLL =  2.2495e+00 RMSE = 1.2618e+00 \n",
      "Samples #    55 : NLL =  2.2504e+00 RMSE = 1.2620e+00 \n",
      "Samples #    60 : NLL =  2.2512e+00 RMSE = 1.2651e+00 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.2448e+00 RMSE = 1.2212e+00 \n",
      "Samples #    70 : NLL =  2.2448e+00 RMSE = 1.2178e+00 \n",
      "Samples #    75 : NLL =  2.2445e+00 RMSE = 1.2084e+00 \n",
      "Samples #    80 : NLL =  2.2459e+00 RMSE = 1.2125e+00 \n",
      "Samples #    85 : NLL =  2.2477e+00 RMSE = 1.2128e+00 \n",
      "Samples #    90 : NLL =  2.2482e+00 RMSE = 1.2148e+00 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.2435e+00 RMSE = 1.1883e+00 \n",
      "Samples #   100 : NLL =  2.2437e+00 RMSE = 1.1859e+00 \n",
      "Samples #   105 : NLL =  2.2442e+00 RMSE = 1.1858e+00 \n",
      "Samples #   110 : NLL =  2.2446e+00 RMSE = 1.1850e+00 \n",
      "Samples #   115 : NLL =  2.2452e+00 RMSE = 1.1866e+00 \n",
      "Samples #   120 : NLL =  2.2460e+00 RMSE = 1.1889e+00 \n",
      "R-hat: mean 1.1155 std 0.1139\n",
      "> RMSE = 2.6100 | NLL = 2.5226\n",
      "Loading split 3 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.1507e+00 RMSE = 1.1546e+00 \n",
      "Samples #    10 : NLL =  2.2057e+00 RMSE = 1.2995e+00 \n",
      "Samples #    15 : NLL =  2.2226e+00 RMSE = 1.3701e+00 \n",
      "Samples #    20 : NLL =  2.2305e+00 RMSE = 1.4018e+00 \n",
      "Samples #    25 : NLL =  2.2362e+00 RMSE = 1.4112e+00 \n",
      "Samples #    30 : NLL =  2.2407e+00 RMSE = 1.4062e+00 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.2280e+00 RMSE = 1.3036e+00 \n",
      "Samples #    40 : NLL =  2.2294e+00 RMSE = 1.2865e+00 \n",
      "Samples #    45 : NLL =  2.2310e+00 RMSE = 1.2845e+00 \n",
      "Samples #    50 : NLL =  2.2329e+00 RMSE = 1.2740e+00 \n",
      "Samples #    55 : NLL =  2.2338e+00 RMSE = 1.2769e+00 \n",
      "Samples #    60 : NLL =  2.2352e+00 RMSE = 1.2813e+00 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.2283e+00 RMSE = 1.2326e+00 \n",
      "Samples #    70 : NLL =  2.2295e+00 RMSE = 1.2295e+00 \n",
      "Samples #    75 : NLL =  2.2314e+00 RMSE = 1.2333e+00 \n",
      "Samples #    80 : NLL =  2.2321e+00 RMSE = 1.2290e+00 \n",
      "Samples #    85 : NLL =  2.2330e+00 RMSE = 1.2293e+00 \n",
      "Samples #    90 : NLL =  2.2334e+00 RMSE = 1.2305e+00 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.2290e+00 RMSE = 1.2021e+00 \n",
      "Samples #   100 : NLL =  2.2296e+00 RMSE = 1.2033e+00 \n",
      "Samples #   105 : NLL =  2.2308e+00 RMSE = 1.2031e+00 \n",
      "Samples #   110 : NLL =  2.2318e+00 RMSE = 1.2070e+00 \n",
      "Samples #   115 : NLL =  2.2325e+00 RMSE = 1.2088e+00 \n",
      "Samples #   120 : NLL =  2.2333e+00 RMSE = 1.2135e+00 \n",
      "R-hat: mean 1.1005 std 0.1061\n",
      "> RMSE = 2.2900 | NLL = 2.4656\n",
      "Loading split 4 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.1183e+00 RMSE = 1.0242e+00 \n",
      "Samples #    10 : NLL =  2.1840e+00 RMSE = 1.2328e+00 \n",
      "Samples #    15 : NLL =  2.1959e+00 RMSE = 1.2493e+00 \n",
      "Samples #    20 : NLL =  2.2055e+00 RMSE = 1.2767e+00 \n",
      "Samples #    25 : NLL =  2.2137e+00 RMSE = 1.3101e+00 \n",
      "Samples #    30 : NLL =  2.2196e+00 RMSE = 1.3374e+00 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.2091e+00 RMSE = 1.2549e+00 \n",
      "Samples #    40 : NLL =  2.2129e+00 RMSE = 1.2456e+00 \n",
      "Samples #    45 : NLL =  2.2179e+00 RMSE = 1.2499e+00 \n",
      "Samples #    50 : NLL =  2.2217e+00 RMSE = 1.2596e+00 \n",
      "Samples #    55 : NLL =  2.2245e+00 RMSE = 1.2697e+00 \n",
      "Samples #    60 : NLL =  2.2266e+00 RMSE = 1.2835e+00 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.2196e+00 RMSE = 1.2444e+00 \n",
      "Samples #    70 : NLL =  2.2207e+00 RMSE = 1.2358e+00 \n",
      "Samples #    75 : NLL =  2.2218e+00 RMSE = 1.2319e+00 \n",
      "Samples #    80 : NLL =  2.2228e+00 RMSE = 1.2253e+00 \n",
      "Samples #    85 : NLL =  2.2241e+00 RMSE = 1.2223e+00 \n",
      "Samples #    90 : NLL =  2.2247e+00 RMSE = 1.2205e+00 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.2200e+00 RMSE = 1.1923e+00 \n",
      "Samples #   100 : NLL =  2.2198e+00 RMSE = 1.1882e+00 \n",
      "Samples #   105 : NLL =  2.2211e+00 RMSE = 1.1862e+00 \n",
      "Samples #   110 : NLL =  2.2216e+00 RMSE = 1.1850e+00 \n",
      "Samples #   115 : NLL =  2.2224e+00 RMSE = 1.1847e+00 \n",
      "Samples #   120 : NLL =  2.2230e+00 RMSE = 1.1861e+00 \n",
      "R-hat: mean 1.0874 std 0.0732\n",
      "> RMSE = 2.6608 | NLL = 2.4730\n",
      "Loading split 5 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.1017e+00 RMSE = 9.9010e-01 \n",
      "Samples #    10 : NLL =  2.1573e+00 RMSE = 1.1474e+00 \n",
      "Samples #    15 : NLL =  2.1758e+00 RMSE = 1.1746e+00 \n",
      "Samples #    20 : NLL =  2.1833e+00 RMSE = 1.1991e+00 \n",
      "Samples #    25 : NLL =  2.1879e+00 RMSE = 1.2095e+00 \n",
      "Samples #    30 : NLL =  2.1926e+00 RMSE = 1.2186e+00 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.1812e+00 RMSE = 1.1481e+00 \n",
      "Samples #    40 : NLL =  2.1869e+00 RMSE = 1.1492e+00 \n",
      "Samples #    45 : NLL =  2.1919e+00 RMSE = 1.1691e+00 \n",
      "Samples #    50 : NLL =  2.1966e+00 RMSE = 1.1922e+00 \n",
      "Samples #    55 : NLL =  2.1992e+00 RMSE = 1.2067e+00 \n",
      "Samples #    60 : NLL =  2.1998e+00 RMSE = 1.2098e+00 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.1934e+00 RMSE = 1.1725e+00 \n",
      "Samples #    70 : NLL =  2.1941e+00 RMSE = 1.1683e+00 \n",
      "Samples #    75 : NLL =  2.1953e+00 RMSE = 1.1641e+00 \n",
      "Samples #    80 : NLL =  2.1972e+00 RMSE = 1.1652e+00 \n",
      "Samples #    85 : NLL =  2.1978e+00 RMSE = 1.1670e+00 \n",
      "Samples #    90 : NLL =  2.1977e+00 RMSE = 1.1632e+00 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.1934e+00 RMSE = 1.1396e+00 \n",
      "Samples #   100 : NLL =  2.1943e+00 RMSE = 1.1434e+00 \n",
      "Samples #   105 : NLL =  2.1943e+00 RMSE = 1.1409e+00 \n",
      "Samples #   110 : NLL =  2.1950e+00 RMSE = 1.1446e+00 \n",
      "Samples #   115 : NLL =  2.1956e+00 RMSE = 1.1459e+00 \n",
      "Samples #   120 : NLL =  2.1956e+00 RMSE = 1.1459e+00 \n",
      "R-hat: mean 1.0819 std 0.0874\n",
      "> RMSE = 3.7420 | NLL = 2.5489\n",
      "Loading split 6 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.1302e+00 RMSE = 1.0577e+00 \n",
      "Samples #    10 : NLL =  2.1821e+00 RMSE = 1.1949e+00 \n",
      "Samples #    15 : NLL =  2.1997e+00 RMSE = 1.2348e+00 \n",
      "Samples #    20 : NLL =  2.2125e+00 RMSE = 1.2609e+00 \n",
      "Samples #    25 : NLL =  2.2155e+00 RMSE = 1.2601e+00 \n",
      "Samples #    30 : NLL =  2.2206e+00 RMSE = 1.2632e+00 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.2113e+00 RMSE = 1.1915e+00 \n",
      "Samples #    40 : NLL =  2.2168e+00 RMSE = 1.1908e+00 \n",
      "Samples #    45 : NLL =  2.2219e+00 RMSE = 1.1982e+00 \n",
      "Samples #    50 : NLL =  2.2258e+00 RMSE = 1.2089e+00 \n",
      "Samples #    55 : NLL =  2.2292e+00 RMSE = 1.2189e+00 \n",
      "Samples #    60 : NLL =  2.2323e+00 RMSE = 1.2353e+00 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.2261e+00 RMSE = 1.1904e+00 \n",
      "Samples #    70 : NLL =  2.2274e+00 RMSE = 1.1915e+00 \n",
      "Samples #    75 : NLL =  2.2277e+00 RMSE = 1.1889e+00 \n",
      "Samples #    80 : NLL =  2.2289e+00 RMSE = 1.1891e+00 \n",
      "Samples #    85 : NLL =  2.2305e+00 RMSE = 1.2011e+00 \n",
      "Samples #    90 : NLL =  2.2320e+00 RMSE = 1.2119e+00 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.2274e+00 RMSE = 1.1836e+00 \n",
      "Samples #   100 : NLL =  2.2281e+00 RMSE = 1.1883e+00 \n",
      "Samples #   105 : NLL =  2.2293e+00 RMSE = 1.1919e+00 \n",
      "Samples #   110 : NLL =  2.2298e+00 RMSE = 1.1949e+00 \n",
      "Samples #   115 : NLL =  2.2305e+00 RMSE = 1.2013e+00 \n",
      "Samples #   120 : NLL =  2.2309e+00 RMSE = 1.2029e+00 \n",
      "R-hat: mean 1.0970 std 0.1137\n",
      "> RMSE = 1.7634 | NLL = 2.4345\n",
      "Loading split 7 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.1730e+00 RMSE = 1.1745e+00 \n",
      "Samples #    10 : NLL =  2.2158e+00 RMSE = 1.2710e+00 \n",
      "Samples #    15 : NLL =  2.2332e+00 RMSE = 1.3231e+00 \n",
      "Samples #    20 : NLL =  2.2458e+00 RMSE = 1.3704e+00 \n",
      "Samples #    25 : NLL =  2.2527e+00 RMSE = 1.3921e+00 \n",
      "Samples #    30 : NLL =  2.2547e+00 RMSE = 1.3966e+00 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.2433e+00 RMSE = 1.3128e+00 \n",
      "Samples #    40 : NLL =  2.2477e+00 RMSE = 1.3090e+00 \n",
      "Samples #    45 : NLL =  2.2519e+00 RMSE = 1.3221e+00 \n",
      "Samples #    50 : NLL =  2.2543e+00 RMSE = 1.3356e+00 \n",
      "Samples #    55 : NLL =  2.2586e+00 RMSE = 1.3417e+00 \n",
      "Samples #    60 : NLL =  2.2610e+00 RMSE = 1.3518e+00 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.2542e+00 RMSE = 1.3052e+00 \n",
      "Samples #    70 : NLL =  2.2563e+00 RMSE = 1.3014e+00 \n",
      "Samples #    75 : NLL =  2.2574e+00 RMSE = 1.3019e+00 \n",
      "Samples #    80 : NLL =  2.2582e+00 RMSE = 1.3060e+00 \n",
      "Samples #    85 : NLL =  2.2603e+00 RMSE = 1.3217e+00 \n",
      "Samples #    90 : NLL =  2.2611e+00 RMSE = 1.3291e+00 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.2563e+00 RMSE = 1.2947e+00 \n",
      "Samples #   100 : NLL =  2.2573e+00 RMSE = 1.2901e+00 \n",
      "Samples #   105 : NLL =  2.2577e+00 RMSE = 1.2901e+00 \n",
      "Samples #   110 : NLL =  2.2586e+00 RMSE = 1.2880e+00 \n",
      "Samples #   115 : NLL =  2.2595e+00 RMSE = 1.2903e+00 \n",
      "Samples #   120 : NLL =  2.2602e+00 RMSE = 1.2917e+00 \n",
      "R-hat: mean 1.1066 std 0.1197\n",
      "> RMSE = 1.9496 | NLL = 2.4204\n",
      "Loading split 8 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.1423e+00 RMSE = 1.1020e+00 \n",
      "Samples #    10 : NLL =  2.1950e+00 RMSE = 1.2559e+00 \n",
      "Samples #    15 : NLL =  2.2098e+00 RMSE = 1.3113e+00 \n",
      "Samples #    20 : NLL =  2.2195e+00 RMSE = 1.3468e+00 \n",
      "Samples #    25 : NLL =  2.2260e+00 RMSE = 1.3629e+00 \n",
      "Samples #    30 : NLL =  2.2330e+00 RMSE = 1.3762e+00 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.2197e+00 RMSE = 1.2683e+00 \n",
      "Samples #    40 : NLL =  2.2229e+00 RMSE = 1.2570e+00 \n",
      "Samples #    45 : NLL =  2.2257e+00 RMSE = 1.2542e+00 \n",
      "Samples #    50 : NLL =  2.2292e+00 RMSE = 1.2669e+00 \n",
      "Samples #    55 : NLL =  2.2306e+00 RMSE = 1.2770e+00 \n",
      "Samples #    60 : NLL =  2.2316e+00 RMSE = 1.2821e+00 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.2249e+00 RMSE = 1.2329e+00 \n",
      "Samples #    70 : NLL =  2.2249e+00 RMSE = 1.2197e+00 \n",
      "Samples #    75 : NLL =  2.2263e+00 RMSE = 1.2191e+00 \n",
      "Samples #    80 : NLL =  2.2272e+00 RMSE = 1.2160e+00 \n",
      "Samples #    85 : NLL =  2.2286e+00 RMSE = 1.2187e+00 \n",
      "Samples #    90 : NLL =  2.2292e+00 RMSE = 1.2203e+00 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.2251e+00 RMSE = 1.1929e+00 \n",
      "Samples #   100 : NLL =  2.2266e+00 RMSE = 1.1952e+00 \n",
      "Samples #   105 : NLL =  2.2279e+00 RMSE = 1.1999e+00 \n",
      "Samples #   110 : NLL =  2.2289e+00 RMSE = 1.1988e+00 \n",
      "Samples #   115 : NLL =  2.2293e+00 RMSE = 1.2000e+00 \n",
      "Samples #   120 : NLL =  2.2303e+00 RMSE = 1.2059e+00 \n",
      "R-hat: mean 1.0890 std 0.0721\n",
      "> RMSE = 3.4931 | NLL = 2.5104\n",
      "Loading split 9 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.1304e+00 RMSE = 1.0137e+00 \n",
      "Samples #    10 : NLL =  2.1794e+00 RMSE = 1.1340e+00 \n",
      "Samples #    15 : NLL =  2.1967e+00 RMSE = 1.1905e+00 \n",
      "Samples #    20 : NLL =  2.2090e+00 RMSE = 1.2199e+00 \n",
      "Samples #    25 : NLL =  2.2150e+00 RMSE = 1.2214e+00 \n",
      "Samples #    30 : NLL =  2.2224e+00 RMSE = 1.2571e+00 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.2094e+00 RMSE = 1.1685e+00 \n",
      "Samples #    40 : NLL =  2.2133e+00 RMSE = 1.1722e+00 \n",
      "Samples #    45 : NLL =  2.2161e+00 RMSE = 1.1757e+00 \n",
      "Samples #    50 : NLL =  2.2177e+00 RMSE = 1.1672e+00 \n",
      "Samples #    55 : NLL =  2.2187e+00 RMSE = 1.1586e+00 \n",
      "Samples #    60 : NLL =  2.2191e+00 RMSE = 1.1543e+00 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.2128e+00 RMSE = 1.1143e+00 \n",
      "Samples #    70 : NLL =  2.2147e+00 RMSE = 1.1169e+00 \n",
      "Samples #    75 : NLL =  2.2160e+00 RMSE = 1.1198e+00 \n",
      "Samples #    80 : NLL =  2.2175e+00 RMSE = 1.1227e+00 \n",
      "Samples #    85 : NLL =  2.2187e+00 RMSE = 1.1276e+00 \n",
      "Samples #    90 : NLL =  2.2200e+00 RMSE = 1.1321e+00 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.2157e+00 RMSE = 1.1052e+00 \n",
      "Samples #   100 : NLL =  2.2159e+00 RMSE = 1.1027e+00 \n",
      "Samples #   105 : NLL =  2.2167e+00 RMSE = 1.1053e+00 \n",
      "Samples #   110 : NLL =  2.2173e+00 RMSE = 1.1050e+00 \n",
      "Samples #   115 : NLL =  2.2178e+00 RMSE = 1.1116e+00 \n",
      "Samples #   120 : NLL =  2.2185e+00 RMSE = 1.1162e+00 \n",
      "R-hat: mean 1.1119 std 0.0944\n",
      "> RMSE = 3.4384 | NLL = 2.6514\n",
      "Loading split 10 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.1477e+00 RMSE = 1.1665e+00 \n",
      "Samples #    10 : NLL =  2.1990e+00 RMSE = 1.3310e+00 \n",
      "Samples #    15 : NLL =  2.2161e+00 RMSE = 1.3999e+00 \n",
      "Samples #    20 : NLL =  2.2244e+00 RMSE = 1.4300e+00 \n",
      "Samples #    25 : NLL =  2.2271e+00 RMSE = 1.4234e+00 \n",
      "Samples #    30 : NLL =  2.2296e+00 RMSE = 1.4316e+00 \n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.2164e+00 RMSE = 1.3285e+00 \n",
      "Samples #    40 : NLL =  2.2180e+00 RMSE = 1.3023e+00 \n",
      "Samples #    45 : NLL =  2.2201e+00 RMSE = 1.2940e+00 \n",
      "Samples #    50 : NLL =  2.2222e+00 RMSE = 1.2894e+00 \n",
      "Samples #    55 : NLL =  2.2231e+00 RMSE = 1.2918e+00 \n",
      "Samples #    60 : NLL =  2.2247e+00 RMSE = 1.2935e+00 \n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.2196e+00 RMSE = 1.2545e+00 \n",
      "Samples #    70 : NLL =  2.2207e+00 RMSE = 1.2466e+00 \n",
      "Samples #    75 : NLL =  2.2227e+00 RMSE = 1.2486e+00 \n",
      "Samples #    80 : NLL =  2.2230e+00 RMSE = 1.2477e+00 \n",
      "Samples #    85 : NLL =  2.2244e+00 RMSE = 1.2542e+00 \n",
      "Samples #    90 : NLL =  2.2257e+00 RMSE = 1.2584e+00 \n",
      "Chain: 4\n",
      "Samples #    95 : NLL =  2.2212e+00 RMSE = 1.2303e+00 \n",
      "Samples #   100 : NLL =  2.2222e+00 RMSE = 1.2271e+00 \n",
      "Samples #   105 : NLL =  2.2235e+00 RMSE = 1.2269e+00 \n",
      "Samples #   110 : NLL =  2.2253e+00 RMSE = 1.2336e+00 \n",
      "Samples #   115 : NLL =  2.2260e+00 RMSE = 1.2395e+00 \n",
      "Samples #   120 : NLL =  2.2264e+00 RMSE = 1.2444e+00 \n",
      "R-hat: mean 1.1704 std 0.1763\n",
      "> RMSE = 4.6284 | NLL = 2.6550\n"
     ]
    }
   ],
   "source": [
    "results = {\"rmse\": [], \"nll\": []}\n",
    "\n",
    "for split_id in range(n_splits):\n",
    "    print(\"Loading split {} of {} dataset\".format(split_id+1, dataset))\n",
    "    saved_dir = os.path.join(out_dir, str(split_id))\n",
    "    \n",
    "    # Load the dataset\n",
    "    X_train, y_train, X_test, y_test = util.load_uci_data(\n",
    "            data_dir, split_id, dataset)\n",
    "    input_dim, output_dim = int(X_train.shape[-1]), 1\n",
    "\n",
    "    # Initialize the neural network and likelihood modules\n",
    "    weight_mask, bias_mask = masks_list[split_id]\n",
    "    net = MLPMasked(input_dim, output_dim, [n_units] * n_hidden, activation_fn, weight_mask, bias_mask)\n",
    "    likelihood = LikGaussian(noise_var)\n",
    "    \n",
    "    # Load the optimized prior\n",
    "    ckpt_path = os.path.join(out_dir, str(split_id), \"ckpts\", \"sparse-it-{}.ckpt\".format(num_iters_sd))\n",
    "    prior = OptimGaussianPrior(ckpt_path)\n",
    "    \n",
    "    # Initialize bayesian neural network with SGHMC sampler\n",
    "    saved_dir = os.path.join(out_dir, str(split_id))\n",
    "    bayes_net = RegressionNetMasked(net, likelihood, prior, saved_dir, n_gpu=0)\n",
    "    \n",
    "    # Start sampling\n",
    "    bayes_net.sample_multi_chains(X_train, y_train, **sampling_configs)\n",
    "    pred_mean, pred_var, preds, raw_preds = bayes_net.predict(X_test, True, True)\n",
    "    r_hat = compute_rhat_regression(raw_preds, sampling_configs[\"num_chains\"])\n",
    "    print(\"R-hat: mean {:.4f} std {:.4f}\".format(float(r_hat.mean()), float(r_hat.std())))\n",
    "\n",
    "    rmse = uncertainty_metrics.rmse(pred_mean, y_test)\n",
    "    nll = uncertainty_metrics.gaussian_nll(y_test, pred_mean, pred_var)\n",
    "    print(\"> RMSE = {:.4f} | NLL = {:.4f}\".format(rmse, nll))\n",
    "    results['rmse'].append(rmse)\n",
    "    results['nll'].append(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_es = pd.DataFrame(results)\n",
    "result_df_es.to_csv(os.path.join(out_dir, \"optim_results_early_stoppping.csv\"), sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WCP for Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_configs = {\n",
    "    \"batch_size\": 32,                 # Mini-batch size\n",
    "    \"num_samples\": 40,                # Total number of samples for each chain \n",
    "    \"n_discarded\": 10,                # Number of the first samples to be discared for each chain\n",
    "    \"num_burn_in_steps\": 2000,         # Number of burn-in steps\n",
    "    \"keep_every\": 2000,                # Thinning interval\n",
    "    \"lr\": 1e-2,                       # Step size\n",
    "    \"num_chains\": 4,                  # Number of chains\n",
    "    \"mdecay\": 1e-2,                   # Momentum coefficient\n",
    "    \"print_every_n_samples\": 5,\n",
    "    \"prevent_overfitting\" : \"Super early stopping\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading split 1 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.0762e+00 RMSE = 7.9183e-01 \n",
      "Samples #    10 : NLL =  2.0763e+00 RMSE = 6.8669e-01 \n",
      "Samples #    15 : NLL =  2.0734e+00 RMSE = 6.3973e-01 \n",
      "Samples #    20 : NLL =  2.0702e+00 RMSE = 5.8937e-01 \n",
      "Samples #    25 : NLL =  2.0688e+00 RMSE = 5.5490e-01 \n",
      "Samples #    30 : NLL =  2.0677e+00 RMSE = 5.2618e-01 \n",
      "Chain: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m bayes_net \u001b[38;5;241m=\u001b[39m RegressionNetMasked(net, likelihood, prior, saved_dir, n_gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Start sampling\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mbayes_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_multi_chains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msampling_configs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m pred_mean, pred_var, preds, raw_preds \u001b[38;5;241m=\u001b[39m bayes_net\u001b[38;5;241m.\u001b[39mpredict(X_test, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m r_hat \u001b[38;5;241m=\u001b[39m compute_rhat_regression(raw_preds, sampling_configs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_chains\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/Universitat/KAUST/Bachelorarbeit/partially-stochastic-functional-prior/Networks/bayes_net_masked.py:247\u001b[0m, in \u001b[0;36mBayesNetMasked.sample_multi_chains\u001b[0;34m(self, x_train, y_train, data_loader, num_samples, num_chains, keep_every, n_discarded, num_burn_in_steps, lr, batch_size, epsilon, mdecay, print_every_n_samples, resample_prior_every, prevent_overfitting)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChain: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(chain\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mreset_parameters()\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m           \u001b[49m\u001b[43mn_discarded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_burn_in_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmdecay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m           \u001b[49m\u001b[43mprint_every_n_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontinue_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m           \u001b[49m\u001b[43mclear_sampled_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m           \u001b[49m\u001b[43mresample_prior_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample_prior_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m           \u001b[49m\u001b[43mprevent_overfitting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprevent_overfitting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_sampled_weights()\n",
      "File \u001b[0;32m~/Documents/Universitat/KAUST/Bachelorarbeit/partially-stochastic-functional-prior/Networks/bayes_net_masked.py:376\u001b[0m, in \u001b[0;36mBayesNetMasked.train\u001b[0;34m(self, x_train, y_train, data_loader, num_samples, keep_every, n_discarded, num_burn_in_steps, lr, batch_size, epsilon, mdecay, print_every_n_samples, continue_training, clear_sampled_weights, resample_prior_every, resample_hyper_prior_burn_in, prevent_overfitting)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mchange_hook(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    375\u001b[0m det_loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 376\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchange_hook_wcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m deterministic_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mchange_hook(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/Universitat/KAUST/Bachelorarbeit/partially-stochastic-functional-prior/Networks/mlp_masked.py:124\u001b[0m, in \u001b[0;36mchange_hook_wcp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    122\u001b[0m         layer\u001b[38;5;241m.\u001b[39mchange_hook(det_training, dropout, p)\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchange_hook_wcp\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    126\u001b[0m         layer\u001b[38;5;241m.\u001b[39mchange_hook_wcp()\n",
      "File \u001b[0;32m~/Documents/Universitat/KAUST/Bachelorarbeit/partially-stochastic-functional-prior/Layers/masked_linear.py:118\u001b[0m, in \u001b[0;36mchange_hook_wcp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m weight_eta = torch.abs(self.W.grad)\n\u001b[1;32m    117\u001b[0m bias_eta = torch.abs(self.b.grad)\n\u001b[0;32m--> 118\u001b[0m # Remove hooks\n\u001b[1;32m    119\u001b[0m self.weight_hook.remove()\n\u001b[1;32m    120\u001b[0m self.bias_hook.remove()\n",
      "File \u001b[0;32m~/Documents/Universitat/KAUST/Bachelorarbeit/partially-stochastic-functional-prior/Layers/masked_linear.py:187\u001b[0m, in \u001b[0;36m_sample_tensor_from_wcp\u001b[0;34m(self, shape, eta)\u001b[0m\n\u001b[1;32m    185\u001b[0m             collected \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m collected \u001b[38;5;241m==\u001b[39m num_samples:\n\u001b[0;32m--> 187\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_m, samples_s\n",
      "File \u001b[0;32m~/Documents/Universitat/KAUST/Bachelorarbeit/partially-stochastic-functional-prior/Layers/masked_linear.py:176\u001b[0m, in \u001b[0;36msample_m_s\u001b[0;34m(num_samples, eta_vals, M, S, epsilon)\u001b[0m\n\u001b[1;32m    169\u001b[0m max_density \u001b[38;5;241m=\u001b[39m wcp_density(\n\u001b[1;32m    170\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    171\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(epsilon, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    172\u001b[0m     eta_expanded\n\u001b[1;32m    173\u001b[0m )\n\u001b[1;32m    175\u001b[0m u \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand_like(p) \u001b[38;5;241m*\u001b[39m max_density\n\u001b[0;32m--> 176\u001b[0m accepted \u001b[38;5;241m=\u001b[39m u \u001b[38;5;241m<\u001b[39m p\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# For each eta value, accept the first valid sample\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(current_batch_size):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = {\"rmse\": [], \"nll\": []}\n",
    "\n",
    "for split_id in range(n_splits):\n",
    "    print(\"Loading split {} of {} dataset\".format(split_id+1, dataset))\n",
    "    saved_dir = os.path.join(out_dir, str(split_id))\n",
    "    \n",
    "    # Load the dataset\n",
    "    X_train, y_train, X_test, y_test = util.load_uci_data(\n",
    "            data_dir, split_id, dataset)\n",
    "    input_dim, output_dim = int(X_train.shape[-1]), 1\n",
    "\n",
    "    # Initialize the neural network and likelihood modules\n",
    "    weight_mask, bias_mask = masks_list[split_id]\n",
    "    net = MLPMasked(input_dim, output_dim, [n_units] * n_hidden, activation_fn, weight_mask, bias_mask)\n",
    "    likelihood = LikGaussian(noise_var)\n",
    "    \n",
    "    # Load the optimized prior\n",
    "    ckpt_path = os.path.join(out_dir, str(split_id), \"ckpts\", \"sparse-it-{}.ckpt\".format(num_iters_sd))\n",
    "    prior = OptimGaussianPrior(ckpt_path)\n",
    "    \n",
    "    # Initialize bayesian neural network with SGHMC sampler\n",
    "    saved_dir = os.path.join(out_dir, str(split_id))\n",
    "    bayes_net = RegressionNetMasked(net, likelihood, prior, saved_dir, n_gpu=0)\n",
    "    \n",
    "    # Start sampling\n",
    "    bayes_net.sample_multi_chains(X_train, y_train, **sampling_configs)\n",
    "    pred_mean, pred_var, preds, raw_preds = bayes_net.predict(X_test, True, True)\n",
    "    r_hat = compute_rhat_regression(raw_preds, sampling_configs[\"num_chains\"])\n",
    "    print(\"R-hat: mean {:.4f} std {:.4f}\".format(float(r_hat.mean()), float(r_hat.std())))\n",
    "\n",
    "    rmse = uncertainty_metrics.rmse(pred_mean, y_test)\n",
    "    nll = uncertainty_metrics.gaussian_nll(y_test, pred_mean, pred_var)\n",
    "    print(\"> RMSE = {:.4f} | NLL = {:.4f}\".format(rmse, nll))\n",
    "    results['rmse'].append(rmse)\n",
    "    results['nll'].append(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_wcp = pd.DataFrame(results)\n",
    "result_df_wcp.to_csv(os.path.join(out_dir, \"optim_results_wcp.csv\"), sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rmse = [result_df_no[\"rmse\"], result_df_do[\"rmse\"], result_df_es[\"rmse\"], result_df_wcp[\"rmse\"]]\n",
    "data_nll = [result_df_no[\"nll\"], result_df_do[\"nll\"], result_df_es[\"nll\"], result_df_wcp[\"nll\"]]\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 3.5))\n",
    "\n",
    "# RMSE box plot\n",
    "axes[0].boxplot(data_rmse, labels=['No prevention', 'Dropout', 'Early Stopping', 'WCP'])\n",
    "axes[0].set_ylabel(\"RMSE $\\downarrow$\")\n",
    "axes[0].set_title(\"RMSE Comparison\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "# NLL box plot\n",
    "axes[1].boxplot(data_nll, labels=['No prevention', 'Dropout', 'Early Stopping', 'WCP'])\n",
    "axes[1].set_ylabel(\"NLL $\\downarrow$\")\n",
    "axes[1].set_title(\"NLL Comparison\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final results no prevention from overfitting:\")\n",
    "print(\"> RMSE: mean {:.4e}; std {:.4e} | NLL: mean {:.4e} std {:.4e}\".format(\n",
    "        float(result_df_no['rmse'].mean()), float(result_df_no['rmse'].std()),\n",
    "        float(result_df_no['nll'].mean()), float(result_df_no['nll'].std())))\n",
    "print(\"\\nFinal results using dropout:\")\n",
    "print(\"> RMSE: mean {:.4e}; std {:.4e} | NLL: mean {:.4e} std {:.4e}\".format(\n",
    "        float(result_df_do['rmse'].mean()), float(result_df_do['rmse'].std()),\n",
    "        float(result_df_do['nll'].mean()), float(result_df_do['nll'].std())))\n",
    "print(\"\\nFinal results using Early Stopping:\")\n",
    "print(\"> RMSE: mean {:.4e}; std {:.4e} | NLL: mean {:.4e} std {:.4e}\".format(\n",
    "        float(result_df_es['rmse'].mean()), float(result_df_es['rmse'].std()),\n",
    "        float(result_df_es['nll'].mean()), float(result_df_es['nll'].std())))\n",
    "print(\"\\nFinal results using WCP:\")\n",
    "print(\"> RMSE: mean {:.4e}; std {:.4e} | NLL: mean {:.4e} std {:.4e}\".format(\n",
    "        float(result_df_wcp['rmse'].mean()), float(result_df_wcp['rmse'].std()),\n",
    "        float(result_df_wcp['nll'].mean()), float(result_df_wcp['nll'].std())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
