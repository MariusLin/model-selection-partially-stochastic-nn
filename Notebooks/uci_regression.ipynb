{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as ticker\n",
    "import pickle\n",
    "import os\n",
    "import warnings \n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Prior_optimization.gpr import GPR\n",
    "from Prior_optimization import kernels, mean_functions\n",
    "from Networks.factorized_gaussian_reparam_mlp import FactorizedGaussianMLPReparameterization\n",
    "from Samplers.likelihoods import LikGaussian\n",
    "from Prior_optimization.priors import OptimGaussianPrior\n",
    "from Utilities.rand_generators import MeasureSetGenerator\n",
    "from Utilities.normalization import normalize_data\n",
    "from Utilities.exp_utils import get_input_range\n",
    "from Metrics.sampling import compute_rhat_regression\n",
    "from Metrics import uncertainty as uncertainty_metrics\n",
    "from Networks.mlp_masked import MLPMasked\n",
    "from Networks.regression_net_masked import RegressionNetMasked\n",
    "from Prior_optimization.optimisation_mapper import PriorOptimisationMapper\n",
    "from Utilities import util\n",
    "from Utilities.priors import LogNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "util.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network architecture\n",
    "n_units = 100\n",
    "n_hidden = 1\n",
    "activation_fn = \"tanh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configurations\n",
    "n_splits = 10\n",
    "dataset = \"boston\"\n",
    "data_dir = \"./data/uci\"\n",
    "noise_var = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda \n",
      "\n",
      "Tesla V100-SXM2-32GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "Number of available GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "n_gpu = 0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device} \\n')\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    n_gpu += torch.cuda.device_count()\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    print('Number of available GPUs:', str(n_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"./exp/uci/optim_gaussian\"\n",
    "util.ensure_dir(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Optimize the Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations for the prior optimization\n",
    "D = 3\n",
    "mapper_batch_size = 256\n",
    "prior_opt_configurations = {\n",
    "    \"n_data\": mapper_batch_size,                            # The batch size \n",
    "    \"num_iters\": 6000,                                      # The number of iterations of the prior optimization\n",
    "    \"lambd\": (torch.tensor([1])/D).to(device),              # The regularization parameters for the layers\n",
    "    \"n_samples\": 128,                                       # The number of function samples\n",
    "    \"lr\": 1e-1,                                             # The learning rate for the optimizer\n",
    "    \"print_every\": 100,                                     # After how many epochs a evaluation should be printed\n",
    "    \"save_ckpt_every\":500,                                  # After how many epochs a checkpoint should be saved\n",
    "    \"shift\": 1,                                             # The shift of the schedule for the regularization of the loss\n",
    "    \"scale\": 12                                             # The scale of the schedule for the regularization of the loss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading split 1 of boston dataset\n",
      ">>> Iteration #   1: Difference from GP 20721.2775 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 100: Difference from GP 22330.0766 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 200: Difference from GP 20639.9855 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 300: Difference from GP 20407.9682 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 400: Difference from GP 19915.3447 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 500: Difference from GP 20473.6837 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 600: Difference from GP 19992.5713 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 700: Difference from GP 20148.1185 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 800: Difference from GP 20593.4099 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 900: Difference from GP 20135.3299 Number of pruned stochastic weights: 5\n",
      ">>> Iteration # 1000: Difference from GP 20280.1295 Number of pruned stochastic weights: 8\n",
      ">>> Iteration # 1100: Difference from GP 20294.7029 Number of pruned stochastic weights: 8\n",
      ">>> Iteration # 1200: Difference from GP 20292.3497 Number of pruned stochastic weights: 11\n",
      ">>> Iteration # 1300: Difference from GP 20490.4721 Number of pruned stochastic weights: 13\n",
      ">>> Iteration # 1400: Difference from GP 20924.5430 Number of pruned stochastic weights: 20\n",
      ">>> Iteration # 1500: Difference from GP 20770.8002 Number of pruned stochastic weights: 26\n",
      ">>> Iteration # 1600: Difference from GP 20646.3454 Number of pruned stochastic weights: 37\n",
      ">>> Iteration # 1700: Difference from GP 20538.7933 Number of pruned stochastic weights: 41\n",
      ">>> Iteration # 1800: Difference from GP 20268.6903 Number of pruned stochastic weights: 49\n",
      ">>> Iteration # 1900: Difference from GP 20788.2880 Number of pruned stochastic weights: 62\n",
      ">>> Iteration # 2000: Difference from GP 20218.3063 Number of pruned stochastic weights: 78\n",
      ">>> Iteration # 2100: Difference from GP 20544.4739 Number of pruned stochastic weights: 86\n",
      ">>> Iteration # 2200: Difference from GP 20396.7832 Number of pruned stochastic weights: 106\n",
      ">>> Iteration # 2300: Difference from GP 20420.2272 Number of pruned stochastic weights: 108\n",
      ">>> Iteration # 2400: Difference from GP 20034.9265 Number of pruned stochastic weights: 112\n",
      ">>> Iteration # 2500: Difference from GP 21269.1884 Number of pruned stochastic weights: 116\n",
      ">>> Iteration # 2600: Difference from GP 20579.3385 Number of pruned stochastic weights: 126\n",
      ">>> Iteration # 2700: Difference from GP 20254.6800 Number of pruned stochastic weights: 133\n",
      ">>> Iteration # 2800: Difference from GP 20277.2784 Number of pruned stochastic weights: 142\n",
      ">>> Iteration # 2900: Difference from GP 20240.4836 Number of pruned stochastic weights: 155\n",
      ">>> Iteration # 3000: Difference from GP 20597.5099 Number of pruned stochastic weights: 168\n",
      ">>> Iteration # 3100: Difference from GP 20284.2612 Number of pruned stochastic weights: 182\n",
      ">>> Iteration # 3200: Difference from GP 20575.1420 Number of pruned stochastic weights: 198\n",
      ">>> Iteration # 3300: Difference from GP 20264.9130 Number of pruned stochastic weights: 210\n",
      ">>> Iteration # 3400: Difference from GP 20712.9003 Number of pruned stochastic weights: 222\n",
      ">>> Iteration # 3500: Difference from GP 20730.8781 Number of pruned stochastic weights: 241\n",
      ">>> Iteration # 3600: Difference from GP 20384.5553 Number of pruned stochastic weights: 251\n",
      ">>> Iteration # 3700: Difference from GP 20288.6808 Number of pruned stochastic weights: 261\n",
      ">>> Iteration # 3800: Difference from GP 20821.8070 Number of pruned stochastic weights: 269\n",
      ">>> Iteration # 3900: Difference from GP 20183.0457 Number of pruned stochastic weights: 287\n",
      ">>> Iteration # 4000: Difference from GP 20048.0963 Number of pruned stochastic weights: 301\n",
      ">>> Iteration # 4100: Difference from GP 19957.4374 Number of pruned stochastic weights: 318\n",
      ">>> Iteration # 4200: Difference from GP 20164.7089 Number of pruned stochastic weights: 328\n",
      ">>> Iteration # 4300: Difference from GP 20056.6730 Number of pruned stochastic weights: 339\n",
      ">>> Iteration # 4400: Difference from GP 20225.4641 Number of pruned stochastic weights: 349\n",
      ">>> Iteration # 4500: Difference from GP 20212.4682 Number of pruned stochastic weights: 357\n",
      ">>> Iteration # 4600: Difference from GP 20834.9702 Number of pruned stochastic weights: 362\n",
      ">>> Iteration # 4700: Difference from GP 20585.4254 Number of pruned stochastic weights: 368\n",
      ">>> Iteration # 4800: Difference from GP 20321.9392 Number of pruned stochastic weights: 372\n",
      ">>> Iteration # 4900: Difference from GP 20537.3735 Number of pruned stochastic weights: 377\n",
      ">>> Iteration # 5000: Difference from GP 20831.1810 Number of pruned stochastic weights: 379\n",
      ">>> Iteration # 5100: Difference from GP 20209.1478 Number of pruned stochastic weights: 383\n",
      ">>> Iteration # 5200: Difference from GP 20058.6662 Number of pruned stochastic weights: 385\n",
      ">>> Iteration # 5300: Difference from GP 20534.3068 Number of pruned stochastic weights: 391\n",
      ">>> Iteration # 5400: Difference from GP 19885.2882 Number of pruned stochastic weights: 391\n",
      ">>> Iteration # 5500: Difference from GP 20300.9059 Number of pruned stochastic weights: 393\n",
      ">>> Iteration # 5600: Difference from GP 20232.2065 Number of pruned stochastic weights: 393\n",
      ">>> Iteration # 5700: Difference from GP 20707.5198 Number of pruned stochastic weights: 396\n",
      ">>> Iteration # 5800: Difference from GP 20622.0940 Number of pruned stochastic weights: 396\n",
      ">>> Iteration # 5900: Difference from GP 20551.6546 Number of pruned stochastic weights: 396\n",
      ">>> Iteration # 6000: Difference from GP 20293.0650 Number of pruned stochastic weights: 396\n",
      "--------------------------------------------------------------------------------\n",
      "Loading split 2 of boston dataset\n",
      ">>> Iteration #   1: Difference from GP 20217.4028 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 100: Difference from GP 24023.6302 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 200: Difference from GP 21081.3607 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 300: Difference from GP 20188.2470 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 400: Difference from GP 20017.7275 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 500: Difference from GP 20024.7883 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 600: Difference from GP 20918.0509 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 700: Difference from GP 20482.7893 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 800: Difference from GP 20510.4519 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 900: Difference from GP 20513.3714 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 1000: Difference from GP 21157.9510 Number of pruned stochastic weights: 5\n",
      ">>> Iteration # 1100: Difference from GP 20677.5852 Number of pruned stochastic weights: 6\n",
      ">>> Iteration # 1200: Difference from GP 20451.7913 Number of pruned stochastic weights: 17\n",
      ">>> Iteration # 1300: Difference from GP 20118.3284 Number of pruned stochastic weights: 18\n",
      ">>> Iteration # 1400: Difference from GP 19863.5721 Number of pruned stochastic weights: 18\n",
      ">>> Iteration # 1500: Difference from GP 20464.2911 Number of pruned stochastic weights: 18\n",
      ">>> Iteration # 1600: Difference from GP 20351.1830 Number of pruned stochastic weights: 19\n",
      ">>> Iteration # 1700: Difference from GP 20350.8173 Number of pruned stochastic weights: 23\n",
      ">>> Iteration # 1800: Difference from GP 21038.4967 Number of pruned stochastic weights: 27\n",
      ">>> Iteration # 1900: Difference from GP 20273.3405 Number of pruned stochastic weights: 44\n",
      ">>> Iteration # 2000: Difference from GP 20431.6269 Number of pruned stochastic weights: 45\n",
      ">>> Iteration # 2100: Difference from GP 20476.3585 Number of pruned stochastic weights: 50\n",
      ">>> Iteration # 2200: Difference from GP 20620.2703 Number of pruned stochastic weights: 54\n",
      ">>> Iteration # 2300: Difference from GP 20068.7286 Number of pruned stochastic weights: 65\n",
      ">>> Iteration # 2400: Difference from GP 20546.4824 Number of pruned stochastic weights: 81\n",
      ">>> Iteration # 2500: Difference from GP 19684.3843 Number of pruned stochastic weights: 89\n",
      ">>> Iteration # 2600: Difference from GP 20495.6469 Number of pruned stochastic weights: 96\n",
      ">>> Iteration # 2700: Difference from GP 20148.8629 Number of pruned stochastic weights: 107\n",
      ">>> Iteration # 2800: Difference from GP 20377.7981 Number of pruned stochastic weights: 118\n",
      ">>> Iteration # 2900: Difference from GP 20411.7198 Number of pruned stochastic weights: 138\n",
      ">>> Iteration # 3000: Difference from GP 20373.2139 Number of pruned stochastic weights: 153\n",
      ">>> Iteration # 3100: Difference from GP 20104.1578 Number of pruned stochastic weights: 157\n",
      ">>> Iteration # 3200: Difference from GP 20651.5604 Number of pruned stochastic weights: 171\n",
      ">>> Iteration # 3300: Difference from GP 20599.2552 Number of pruned stochastic weights: 183\n",
      ">>> Iteration # 3400: Difference from GP 20203.7744 Number of pruned stochastic weights: 192\n",
      ">>> Iteration # 3500: Difference from GP 19955.2814 Number of pruned stochastic weights: 205\n",
      ">>> Iteration # 3600: Difference from GP 20426.9609 Number of pruned stochastic weights: 226\n",
      ">>> Iteration # 3700: Difference from GP 20908.5824 Number of pruned stochastic weights: 242\n",
      ">>> Iteration # 3800: Difference from GP 20358.3899 Number of pruned stochastic weights: 254\n",
      ">>> Iteration # 3900: Difference from GP 20389.4288 Number of pruned stochastic weights: 261\n",
      ">>> Iteration # 4000: Difference from GP 20235.7927 Number of pruned stochastic weights: 268\n",
      ">>> Iteration # 4100: Difference from GP 20624.7254 Number of pruned stochastic weights: 275\n",
      ">>> Iteration # 4200: Difference from GP 20532.1896 Number of pruned stochastic weights: 278\n",
      ">>> Iteration # 4300: Difference from GP 21515.3376 Number of pruned stochastic weights: 284\n",
      ">>> Iteration # 4400: Difference from GP 20155.4277 Number of pruned stochastic weights: 287\n",
      ">>> Iteration # 4500: Difference from GP 20288.1784 Number of pruned stochastic weights: 293\n",
      ">>> Iteration # 4600: Difference from GP 19882.0883 Number of pruned stochastic weights: 297\n",
      ">>> Iteration # 4700: Difference from GP 20308.0965 Number of pruned stochastic weights: 301\n",
      ">>> Iteration # 4800: Difference from GP 21400.2663 Number of pruned stochastic weights: 307\n",
      ">>> Iteration # 4900: Difference from GP 20460.7028 Number of pruned stochastic weights: 312\n",
      ">>> Iteration # 5000: Difference from GP 20663.2637 Number of pruned stochastic weights: 314\n",
      ">>> Iteration # 5100: Difference from GP 20762.9764 Number of pruned stochastic weights: 316\n",
      ">>> Iteration # 5200: Difference from GP 20581.1584 Number of pruned stochastic weights: 318\n",
      ">>> Iteration # 5300: Difference from GP 20777.4349 Number of pruned stochastic weights: 320\n",
      ">>> Iteration # 5400: Difference from GP 19811.6978 Number of pruned stochastic weights: 321\n",
      ">>> Iteration # 5500: Difference from GP 20338.2015 Number of pruned stochastic weights: 322\n",
      ">>> Iteration # 5600: Difference from GP 20435.7299 Number of pruned stochastic weights: 322\n",
      ">>> Iteration # 5700: Difference from GP 20251.1915 Number of pruned stochastic weights: 322\n",
      ">>> Iteration # 5800: Difference from GP 20640.5179 Number of pruned stochastic weights: 322\n",
      ">>> Iteration # 5900: Difference from GP 20105.6154 Number of pruned stochastic weights: 323\n",
      ">>> Iteration # 6000: Difference from GP 20258.4551 Number of pruned stochastic weights: 323\n",
      "--------------------------------------------------------------------------------\n",
      "Loading split 3 of boston dataset\n",
      ">>> Iteration #   1: Difference from GP 20148.3943 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 100: Difference from GP 20398.7067 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 200: Difference from GP 19906.3976 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 300: Difference from GP 20003.4282 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 400: Difference from GP 20826.6936 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 500: Difference from GP 19883.0984 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 600: Difference from GP 19534.1471 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 700: Difference from GP 20514.6274 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 800: Difference from GP 20152.7107 Number of pruned stochastic weights: 3\n",
      ">>> Iteration # 900: Difference from GP 20810.8421 Number of pruned stochastic weights: 3\n",
      ">>> Iteration # 1000: Difference from GP 19807.3350 Number of pruned stochastic weights: 5\n",
      ">>> Iteration # 1100: Difference from GP 19824.9456 Number of pruned stochastic weights: 9\n",
      ">>> Iteration # 1200: Difference from GP 20727.5596 Number of pruned stochastic weights: 11\n",
      ">>> Iteration # 1300: Difference from GP 20524.2897 Number of pruned stochastic weights: 17\n",
      ">>> Iteration # 1400: Difference from GP 19667.3612 Number of pruned stochastic weights: 27\n",
      ">>> Iteration # 1500: Difference from GP 20818.1936 Number of pruned stochastic weights: 29\n",
      ">>> Iteration # 1600: Difference from GP 19529.4540 Number of pruned stochastic weights: 29\n",
      ">>> Iteration # 1700: Difference from GP 20888.8937 Number of pruned stochastic weights: 30\n",
      ">>> Iteration # 1800: Difference from GP 19654.1729 Number of pruned stochastic weights: 31\n",
      ">>> Iteration # 1900: Difference from GP 19880.0685 Number of pruned stochastic weights: 31\n",
      ">>> Iteration # 2000: Difference from GP 20219.2623 Number of pruned stochastic weights: 34\n",
      ">>> Iteration # 2100: Difference from GP 20093.7757 Number of pruned stochastic weights: 39\n",
      ">>> Iteration # 2200: Difference from GP 20233.8921 Number of pruned stochastic weights: 44\n",
      ">>> Iteration # 2300: Difference from GP 19653.4616 Number of pruned stochastic weights: 68\n",
      ">>> Iteration # 2400: Difference from GP 19834.3093 Number of pruned stochastic weights: 69\n",
      ">>> Iteration # 2500: Difference from GP 19900.0200 Number of pruned stochastic weights: 71\n",
      ">>> Iteration # 2600: Difference from GP 20854.3203 Number of pruned stochastic weights: 81\n",
      ">>> Iteration # 2700: Difference from GP 20126.2021 Number of pruned stochastic weights: 83\n",
      ">>> Iteration # 2800: Difference from GP 20380.6436 Number of pruned stochastic weights: 92\n",
      ">>> Iteration # 2900: Difference from GP 20347.4668 Number of pruned stochastic weights: 100\n",
      ">>> Iteration # 3000: Difference from GP 19603.7203 Number of pruned stochastic weights: 112\n",
      ">>> Iteration # 3100: Difference from GP 20645.1842 Number of pruned stochastic weights: 125\n",
      ">>> Iteration # 3200: Difference from GP 20542.1356 Number of pruned stochastic weights: 135\n",
      ">>> Iteration # 3300: Difference from GP 20135.2817 Number of pruned stochastic weights: 144\n",
      ">>> Iteration # 3400: Difference from GP 20182.3260 Number of pruned stochastic weights: 157\n",
      ">>> Iteration # 3500: Difference from GP 19871.7249 Number of pruned stochastic weights: 174\n",
      ">>> Iteration # 3600: Difference from GP 19907.6220 Number of pruned stochastic weights: 190\n",
      ">>> Iteration # 3700: Difference from GP 20219.5530 Number of pruned stochastic weights: 200\n",
      ">>> Iteration # 3800: Difference from GP 20049.8976 Number of pruned stochastic weights: 209\n",
      ">>> Iteration # 3900: Difference from GP 20641.7850 Number of pruned stochastic weights: 219\n",
      ">>> Iteration # 4000: Difference from GP 19960.7196 Number of pruned stochastic weights: 233\n",
      ">>> Iteration # 4100: Difference from GP 20262.3591 Number of pruned stochastic weights: 245\n",
      ">>> Iteration # 4200: Difference from GP 19776.9999 Number of pruned stochastic weights: 251\n",
      ">>> Iteration # 4300: Difference from GP 19961.5892 Number of pruned stochastic weights: 258\n",
      ">>> Iteration # 4400: Difference from GP 20122.1389 Number of pruned stochastic weights: 264\n",
      ">>> Iteration # 4500: Difference from GP 20070.5652 Number of pruned stochastic weights: 267\n",
      ">>> Iteration # 4600: Difference from GP 19828.2944 Number of pruned stochastic weights: 272\n",
      ">>> Iteration # 4700: Difference from GP 19730.1030 Number of pruned stochastic weights: 275\n",
      ">>> Iteration # 4800: Difference from GP 19881.3005 Number of pruned stochastic weights: 278\n",
      ">>> Iteration # 4900: Difference from GP 20484.3855 Number of pruned stochastic weights: 282\n",
      ">>> Iteration # 5000: Difference from GP 19681.6048 Number of pruned stochastic weights: 287\n",
      ">>> Iteration # 5100: Difference from GP 20078.2089 Number of pruned stochastic weights: 290\n",
      ">>> Iteration # 5200: Difference from GP 20186.6775 Number of pruned stochastic weights: 292\n",
      ">>> Iteration # 5300: Difference from GP 20516.4776 Number of pruned stochastic weights: 295\n",
      ">>> Iteration # 5400: Difference from GP 19614.6034 Number of pruned stochastic weights: 298\n",
      ">>> Iteration # 5500: Difference from GP 20377.3989 Number of pruned stochastic weights: 299\n",
      ">>> Iteration # 5600: Difference from GP 19703.6679 Number of pruned stochastic weights: 302\n",
      ">>> Iteration # 5700: Difference from GP 19942.4894 Number of pruned stochastic weights: 302\n",
      ">>> Iteration # 5800: Difference from GP 20616.7242 Number of pruned stochastic weights: 302\n",
      ">>> Iteration # 5900: Difference from GP 19882.8253 Number of pruned stochastic weights: 303\n",
      ">>> Iteration # 6000: Difference from GP 20221.5856 Number of pruned stochastic weights: 303\n",
      "--------------------------------------------------------------------------------\n",
      "Loading split 4 of boston dataset\n",
      ">>> Iteration #   1: Difference from GP 20332.5674 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 100: Difference from GP 25644.3825 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 200: Difference from GP 23495.8965 Number of pruned stochastic weights: 3\n",
      ">>> Iteration # 300: Difference from GP 23237.7631 Number of pruned stochastic weights: 3\n",
      ">>> Iteration # 400: Difference from GP 20824.9746 Number of pruned stochastic weights: 3\n",
      ">>> Iteration # 500: Difference from GP 20982.3899 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 600: Difference from GP 20520.5764 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 700: Difference from GP 20393.2480 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 800: Difference from GP 20619.3850 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 900: Difference from GP 20015.9503 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 1000: Difference from GP 20726.1628 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 1100: Difference from GP 20637.4470 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 1200: Difference from GP 21000.9574 Number of pruned stochastic weights: 5\n",
      ">>> Iteration # 1300: Difference from GP 20538.0066 Number of pruned stochastic weights: 7\n",
      ">>> Iteration # 1400: Difference from GP 20710.5779 Number of pruned stochastic weights: 8\n",
      ">>> Iteration # 1500: Difference from GP 20250.9469 Number of pruned stochastic weights: 10\n",
      ">>> Iteration # 1600: Difference from GP 20018.5962 Number of pruned stochastic weights: 14\n",
      ">>> Iteration # 1700: Difference from GP 20337.5056 Number of pruned stochastic weights: 22\n",
      ">>> Iteration # 1800: Difference from GP 20310.9066 Number of pruned stochastic weights: 38\n",
      ">>> Iteration # 1900: Difference from GP 20556.4097 Number of pruned stochastic weights: 46\n",
      ">>> Iteration # 2000: Difference from GP 20379.3641 Number of pruned stochastic weights: 47\n",
      ">>> Iteration # 2100: Difference from GP 20674.1593 Number of pruned stochastic weights: 47\n",
      ">>> Iteration # 2200: Difference from GP 21273.7582 Number of pruned stochastic weights: 48\n",
      ">>> Iteration # 2300: Difference from GP 19830.6513 Number of pruned stochastic weights: 54\n",
      ">>> Iteration # 2400: Difference from GP 20278.9920 Number of pruned stochastic weights: 61\n",
      ">>> Iteration # 2500: Difference from GP 20586.2511 Number of pruned stochastic weights: 64\n",
      ">>> Iteration # 2600: Difference from GP 20578.1673 Number of pruned stochastic weights: 72\n",
      ">>> Iteration # 2700: Difference from GP 20582.7829 Number of pruned stochastic weights: 77\n",
      ">>> Iteration # 2800: Difference from GP 20643.8113 Number of pruned stochastic weights: 95\n",
      ">>> Iteration # 2900: Difference from GP 20104.9849 Number of pruned stochastic weights: 113\n",
      ">>> Iteration # 3000: Difference from GP 20797.1943 Number of pruned stochastic weights: 128\n",
      ">>> Iteration # 3100: Difference from GP 19746.7624 Number of pruned stochastic weights: 133\n",
      ">>> Iteration # 3200: Difference from GP 20588.9654 Number of pruned stochastic weights: 142\n",
      ">>> Iteration # 3300: Difference from GP 20617.8852 Number of pruned stochastic weights: 153\n",
      ">>> Iteration # 3400: Difference from GP 20283.8077 Number of pruned stochastic weights: 160\n",
      ">>> Iteration # 3500: Difference from GP 20219.4412 Number of pruned stochastic weights: 170\n",
      ">>> Iteration # 3600: Difference from GP 21058.1334 Number of pruned stochastic weights: 179\n",
      ">>> Iteration # 3700: Difference from GP 20340.5384 Number of pruned stochastic weights: 184\n",
      ">>> Iteration # 3800: Difference from GP 20532.8457 Number of pruned stochastic weights: 198\n",
      ">>> Iteration # 3900: Difference from GP 20202.3502 Number of pruned stochastic weights: 211\n",
      ">>> Iteration # 4000: Difference from GP 20301.5101 Number of pruned stochastic weights: 222\n",
      ">>> Iteration # 4100: Difference from GP 19640.6014 Number of pruned stochastic weights: 229\n",
      ">>> Iteration # 4200: Difference from GP 19715.1333 Number of pruned stochastic weights: 238\n",
      ">>> Iteration # 4300: Difference from GP 20242.5367 Number of pruned stochastic weights: 247\n",
      ">>> Iteration # 4400: Difference from GP 20159.2841 Number of pruned stochastic weights: 255\n",
      ">>> Iteration # 4500: Difference from GP 20441.6157 Number of pruned stochastic weights: 262\n",
      ">>> Iteration # 4600: Difference from GP 21426.6162 Number of pruned stochastic weights: 270\n",
      ">>> Iteration # 4700: Difference from GP 20563.9037 Number of pruned stochastic weights: 273\n",
      ">>> Iteration # 4800: Difference from GP 20372.4193 Number of pruned stochastic weights: 281\n",
      ">>> Iteration # 4900: Difference from GP 20558.7887 Number of pruned stochastic weights: 287\n",
      ">>> Iteration # 5000: Difference from GP 21016.7088 Number of pruned stochastic weights: 290\n",
      ">>> Iteration # 5100: Difference from GP 20229.8132 Number of pruned stochastic weights: 292\n",
      ">>> Iteration # 5200: Difference from GP 20273.8786 Number of pruned stochastic weights: 293\n",
      ">>> Iteration # 5300: Difference from GP 20320.5328 Number of pruned stochastic weights: 294\n",
      ">>> Iteration # 5400: Difference from GP 20441.4056 Number of pruned stochastic weights: 295\n",
      ">>> Iteration # 5500: Difference from GP 20815.2428 Number of pruned stochastic weights: 299\n",
      ">>> Iteration # 5600: Difference from GP 19804.1380 Number of pruned stochastic weights: 301\n",
      ">>> Iteration # 5700: Difference from GP 20755.8831 Number of pruned stochastic weights: 301\n",
      ">>> Iteration # 5800: Difference from GP 20396.5316 Number of pruned stochastic weights: 301\n",
      ">>> Iteration # 5900: Difference from GP 20790.5268 Number of pruned stochastic weights: 301\n",
      ">>> Iteration # 6000: Difference from GP 20365.0009 Number of pruned stochastic weights: 301\n",
      "--------------------------------------------------------------------------------\n",
      "Loading split 5 of boston dataset\n",
      ">>> Iteration #   1: Difference from GP 20189.4795 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 100: Difference from GP 20967.1910 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 200: Difference from GP 20545.0578 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 300: Difference from GP 20000.8169 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 400: Difference from GP 19704.2302 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 500: Difference from GP 19814.9003 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 600: Difference from GP 20473.7820 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 700: Difference from GP 19443.0232 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 800: Difference from GP 19843.7780 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 900: Difference from GP 20683.0416 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 1000: Difference from GP 19842.9285 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 1100: Difference from GP 20023.7514 Number of pruned stochastic weights: 5\n",
      ">>> Iteration # 1200: Difference from GP 20226.8256 Number of pruned stochastic weights: 5\n",
      ">>> Iteration # 1300: Difference from GP 20088.4357 Number of pruned stochastic weights: 10\n",
      ">>> Iteration # 1400: Difference from GP 20205.4948 Number of pruned stochastic weights: 12\n",
      ">>> Iteration # 1500: Difference from GP 21262.3356 Number of pruned stochastic weights: 21\n",
      ">>> Iteration # 1600: Difference from GP 20181.6262 Number of pruned stochastic weights: 23\n",
      ">>> Iteration # 1700: Difference from GP 19936.5340 Number of pruned stochastic weights: 25\n",
      ">>> Iteration # 1800: Difference from GP 20694.5189 Number of pruned stochastic weights: 29\n",
      ">>> Iteration # 1900: Difference from GP 19547.6675 Number of pruned stochastic weights: 37\n",
      ">>> Iteration # 2000: Difference from GP 20060.6494 Number of pruned stochastic weights: 56\n",
      ">>> Iteration # 2100: Difference from GP 20311.6801 Number of pruned stochastic weights: 73\n",
      ">>> Iteration # 2200: Difference from GP 19827.2444 Number of pruned stochastic weights: 90\n",
      ">>> Iteration # 2300: Difference from GP 19962.5762 Number of pruned stochastic weights: 104\n",
      ">>> Iteration # 2400: Difference from GP 20792.8858 Number of pruned stochastic weights: 121\n",
      ">>> Iteration # 2500: Difference from GP 20317.4860 Number of pruned stochastic weights: 136\n",
      ">>> Iteration # 2600: Difference from GP 20428.2770 Number of pruned stochastic weights: 144\n",
      ">>> Iteration # 2700: Difference from GP 20154.9774 Number of pruned stochastic weights: 160\n",
      ">>> Iteration # 2800: Difference from GP 19649.4482 Number of pruned stochastic weights: 170\n",
      ">>> Iteration # 2900: Difference from GP 20321.0883 Number of pruned stochastic weights: 184\n",
      ">>> Iteration # 3000: Difference from GP 20529.5113 Number of pruned stochastic weights: 194\n",
      ">>> Iteration # 3100: Difference from GP 19852.8352 Number of pruned stochastic weights: 207\n",
      ">>> Iteration # 3200: Difference from GP 21115.0855 Number of pruned stochastic weights: 219\n",
      ">>> Iteration # 3300: Difference from GP 20236.5857 Number of pruned stochastic weights: 232\n",
      ">>> Iteration # 3400: Difference from GP 20485.4165 Number of pruned stochastic weights: 245\n",
      ">>> Iteration # 3500: Difference from GP 20478.2573 Number of pruned stochastic weights: 256\n",
      ">>> Iteration # 3600: Difference from GP 20477.7676 Number of pruned stochastic weights: 265\n",
      ">>> Iteration # 3700: Difference from GP 20632.4776 Number of pruned stochastic weights: 276\n",
      ">>> Iteration # 3800: Difference from GP 21134.3412 Number of pruned stochastic weights: 294\n",
      ">>> Iteration # 3900: Difference from GP 20397.6932 Number of pruned stochastic weights: 302\n",
      ">>> Iteration # 4000: Difference from GP 20522.9000 Number of pruned stochastic weights: 316\n",
      ">>> Iteration # 4100: Difference from GP 20809.6817 Number of pruned stochastic weights: 326\n",
      ">>> Iteration # 4200: Difference from GP 20097.3939 Number of pruned stochastic weights: 330\n",
      ">>> Iteration # 4300: Difference from GP 20191.1593 Number of pruned stochastic weights: 339\n",
      ">>> Iteration # 4400: Difference from GP 19505.1180 Number of pruned stochastic weights: 349\n",
      ">>> Iteration # 4500: Difference from GP 20388.4805 Number of pruned stochastic weights: 354\n",
      ">>> Iteration # 4600: Difference from GP 19456.2496 Number of pruned stochastic weights: 359\n",
      ">>> Iteration # 4700: Difference from GP 20702.6459 Number of pruned stochastic weights: 363\n",
      ">>> Iteration # 4800: Difference from GP 19926.8477 Number of pruned stochastic weights: 366\n",
      ">>> Iteration # 4900: Difference from GP 20732.7605 Number of pruned stochastic weights: 370\n",
      ">>> Iteration # 5000: Difference from GP 19891.9325 Number of pruned stochastic weights: 378\n",
      ">>> Iteration # 5100: Difference from GP 20302.6186 Number of pruned stochastic weights: 383\n",
      ">>> Iteration # 5200: Difference from GP 19747.0553 Number of pruned stochastic weights: 385\n",
      ">>> Iteration # 5300: Difference from GP 20355.9423 Number of pruned stochastic weights: 385\n",
      ">>> Iteration # 5400: Difference from GP 20436.2913 Number of pruned stochastic weights: 386\n",
      ">>> Iteration # 5500: Difference from GP 19780.4130 Number of pruned stochastic weights: 389\n",
      ">>> Iteration # 5600: Difference from GP 19810.8264 Number of pruned stochastic weights: 390\n",
      ">>> Iteration # 5700: Difference from GP 20144.5806 Number of pruned stochastic weights: 391\n",
      ">>> Iteration # 5800: Difference from GP 20275.5636 Number of pruned stochastic weights: 391\n",
      ">>> Iteration # 5900: Difference from GP 20504.8580 Number of pruned stochastic weights: 391\n",
      ">>> Iteration # 6000: Difference from GP 19815.2053 Number of pruned stochastic weights: 391\n",
      "--------------------------------------------------------------------------------\n",
      "Loading split 6 of boston dataset\n",
      ">>> Iteration #   1: Difference from GP 20530.6016 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 100: Difference from GP 40551.0045 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 200: Difference from GP 21083.1868 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 300: Difference from GP 20243.5830 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 400: Difference from GP 20386.5174 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 500: Difference from GP 19671.3267 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 600: Difference from GP 20304.8452 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 700: Difference from GP 20307.7166 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 800: Difference from GP 19612.9535 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 900: Difference from GP 20701.5351 Number of pruned stochastic weights: 2\n",
      ">>> Iteration # 1000: Difference from GP 19995.7621 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 1100: Difference from GP 20931.0364 Number of pruned stochastic weights: 12\n",
      ">>> Iteration # 1200: Difference from GP 20370.6326 Number of pruned stochastic weights: 12\n",
      ">>> Iteration # 1300: Difference from GP 20367.3409 Number of pruned stochastic weights: 12\n",
      ">>> Iteration # 1400: Difference from GP 19647.1103 Number of pruned stochastic weights: 12\n",
      ">>> Iteration # 1500: Difference from GP 20650.6452 Number of pruned stochastic weights: 12\n",
      ">>> Iteration # 1600: Difference from GP 20978.5317 Number of pruned stochastic weights: 13\n",
      ">>> Iteration # 1700: Difference from GP 20799.7524 Number of pruned stochastic weights: 20\n",
      ">>> Iteration # 1800: Difference from GP 20308.2107 Number of pruned stochastic weights: 23\n",
      ">>> Iteration # 1900: Difference from GP 20355.7739 Number of pruned stochastic weights: 30\n",
      ">>> Iteration # 2000: Difference from GP 20588.2785 Number of pruned stochastic weights: 46\n",
      ">>> Iteration # 2100: Difference from GP 20439.6611 Number of pruned stochastic weights: 61\n",
      ">>> Iteration # 2200: Difference from GP 20722.8135 Number of pruned stochastic weights: 76\n",
      ">>> Iteration # 2300: Difference from GP 20518.2091 Number of pruned stochastic weights: 82\n",
      ">>> Iteration # 2400: Difference from GP 20318.4224 Number of pruned stochastic weights: 98\n",
      ">>> Iteration # 2500: Difference from GP 20635.6494 Number of pruned stochastic weights: 118\n",
      ">>> Iteration # 2600: Difference from GP 20074.6596 Number of pruned stochastic weights: 134\n",
      ">>> Iteration # 2700: Difference from GP 20543.1269 Number of pruned stochastic weights: 167\n",
      ">>> Iteration # 2800: Difference from GP 20489.6312 Number of pruned stochastic weights: 174\n",
      ">>> Iteration # 2900: Difference from GP 20271.8465 Number of pruned stochastic weights: 192\n",
      ">>> Iteration # 3000: Difference from GP 19522.3264 Number of pruned stochastic weights: 200\n",
      ">>> Iteration # 3100: Difference from GP 20367.5975 Number of pruned stochastic weights: 214\n",
      ">>> Iteration # 3200: Difference from GP 20054.9919 Number of pruned stochastic weights: 221\n",
      ">>> Iteration # 3300: Difference from GP 20714.7744 Number of pruned stochastic weights: 235\n",
      ">>> Iteration # 3400: Difference from GP 20360.2353 Number of pruned stochastic weights: 251\n",
      ">>> Iteration # 3500: Difference from GP 20309.6957 Number of pruned stochastic weights: 264\n",
      ">>> Iteration # 3600: Difference from GP 20229.1329 Number of pruned stochastic weights: 276\n",
      ">>> Iteration # 3700: Difference from GP 20445.2760 Number of pruned stochastic weights: 285\n",
      ">>> Iteration # 3800: Difference from GP 20515.2521 Number of pruned stochastic weights: 298\n",
      ">>> Iteration # 3900: Difference from GP 20237.6591 Number of pruned stochastic weights: 313\n",
      ">>> Iteration # 4000: Difference from GP 20437.8767 Number of pruned stochastic weights: 322\n",
      ">>> Iteration # 4100: Difference from GP 20310.2874 Number of pruned stochastic weights: 331\n",
      ">>> Iteration # 4200: Difference from GP 20787.2246 Number of pruned stochastic weights: 341\n",
      ">>> Iteration # 4300: Difference from GP 19977.1672 Number of pruned stochastic weights: 352\n",
      ">>> Iteration # 4400: Difference from GP 20558.3991 Number of pruned stochastic weights: 361\n",
      ">>> Iteration # 4500: Difference from GP 20947.3313 Number of pruned stochastic weights: 371\n",
      ">>> Iteration # 4600: Difference from GP 20169.2861 Number of pruned stochastic weights: 378\n",
      ">>> Iteration # 4700: Difference from GP 20644.4357 Number of pruned stochastic weights: 383\n",
      ">>> Iteration # 4800: Difference from GP 21157.0429 Number of pruned stochastic weights: 386\n",
      ">>> Iteration # 4900: Difference from GP 21299.4287 Number of pruned stochastic weights: 390\n",
      ">>> Iteration # 5000: Difference from GP 20152.6722 Number of pruned stochastic weights: 392\n",
      ">>> Iteration # 5100: Difference from GP 20036.6122 Number of pruned stochastic weights: 395\n",
      ">>> Iteration # 5200: Difference from GP 19989.8365 Number of pruned stochastic weights: 398\n",
      ">>> Iteration # 5300: Difference from GP 20244.8315 Number of pruned stochastic weights: 402\n",
      ">>> Iteration # 5400: Difference from GP 20973.0623 Number of pruned stochastic weights: 403\n",
      ">>> Iteration # 5500: Difference from GP 20532.0182 Number of pruned stochastic weights: 403\n",
      ">>> Iteration # 5600: Difference from GP 20267.5764 Number of pruned stochastic weights: 403\n",
      ">>> Iteration # 5700: Difference from GP 21017.6097 Number of pruned stochastic weights: 406\n",
      ">>> Iteration # 5800: Difference from GP 21064.9817 Number of pruned stochastic weights: 406\n",
      ">>> Iteration # 5900: Difference from GP 20362.7488 Number of pruned stochastic weights: 406\n",
      ">>> Iteration # 6000: Difference from GP 20313.1000 Number of pruned stochastic weights: 406\n",
      "--------------------------------------------------------------------------------\n",
      "Loading split 7 of boston dataset\n",
      ">>> Iteration #   1: Difference from GP 20715.9766 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 100: Difference from GP 20506.7601 Number of pruned stochastic weights: 3\n",
      ">>> Iteration # 200: Difference from GP 19702.6733 Number of pruned stochastic weights: 3\n",
      ">>> Iteration # 300: Difference from GP 21870.9520 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 400: Difference from GP 20289.3129 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 500: Difference from GP 20363.1741 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 600: Difference from GP 20527.7099 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 700: Difference from GP 20105.8399 Number of pruned stochastic weights: 5\n",
      ">>> Iteration # 800: Difference from GP 20340.9804 Number of pruned stochastic weights: 7\n",
      ">>> Iteration # 900: Difference from GP 20216.0037 Number of pruned stochastic weights: 9\n",
      ">>> Iteration # 1000: Difference from GP 20492.1022 Number of pruned stochastic weights: 10\n",
      ">>> Iteration # 1100: Difference from GP 19718.3458 Number of pruned stochastic weights: 14\n",
      ">>> Iteration # 1200: Difference from GP 21028.8487 Number of pruned stochastic weights: 31\n",
      ">>> Iteration # 1300: Difference from GP 20990.0121 Number of pruned stochastic weights: 33\n",
      ">>> Iteration # 1400: Difference from GP 19577.9856 Number of pruned stochastic weights: 33\n",
      ">>> Iteration # 1500: Difference from GP 19595.8168 Number of pruned stochastic weights: 33\n",
      ">>> Iteration # 1600: Difference from GP 19924.6504 Number of pruned stochastic weights: 36\n",
      ">>> Iteration # 1700: Difference from GP 20048.5821 Number of pruned stochastic weights: 38\n",
      ">>> Iteration # 1800: Difference from GP 20569.8280 Number of pruned stochastic weights: 41\n",
      ">>> Iteration # 1900: Difference from GP 19844.2193 Number of pruned stochastic weights: 45\n",
      ">>> Iteration # 2000: Difference from GP 22279.9448 Number of pruned stochastic weights: 48\n",
      ">>> Iteration # 2100: Difference from GP 20506.2340 Number of pruned stochastic weights: 57\n",
      ">>> Iteration # 2200: Difference from GP 20416.6104 Number of pruned stochastic weights: 66\n",
      ">>> Iteration # 2300: Difference from GP 20561.9843 Number of pruned stochastic weights: 82\n",
      ">>> Iteration # 2400: Difference from GP 20189.3415 Number of pruned stochastic weights: 90\n",
      ">>> Iteration # 2500: Difference from GP 20935.5029 Number of pruned stochastic weights: 102\n",
      ">>> Iteration # 2600: Difference from GP 19974.7468 Number of pruned stochastic weights: 118\n",
      ">>> Iteration # 2700: Difference from GP 20305.4194 Number of pruned stochastic weights: 126\n",
      ">>> Iteration # 2800: Difference from GP 19968.3204 Number of pruned stochastic weights: 134\n",
      ">>> Iteration # 2900: Difference from GP 20046.0338 Number of pruned stochastic weights: 154\n",
      ">>> Iteration # 3000: Difference from GP 20454.8065 Number of pruned stochastic weights: 175\n",
      ">>> Iteration # 3100: Difference from GP 19614.5875 Number of pruned stochastic weights: 192\n",
      ">>> Iteration # 3200: Difference from GP 20201.4612 Number of pruned stochastic weights: 206\n",
      ">>> Iteration # 3300: Difference from GP 20498.1592 Number of pruned stochastic weights: 215\n",
      ">>> Iteration # 3400: Difference from GP 20009.6199 Number of pruned stochastic weights: 231\n",
      ">>> Iteration # 3500: Difference from GP 19691.4991 Number of pruned stochastic weights: 239\n",
      ">>> Iteration # 3600: Difference from GP 20000.8825 Number of pruned stochastic weights: 249\n",
      ">>> Iteration # 3700: Difference from GP 20042.3421 Number of pruned stochastic weights: 261\n",
      ">>> Iteration # 3800: Difference from GP 19924.6751 Number of pruned stochastic weights: 270\n",
      ">>> Iteration # 3900: Difference from GP 20441.7401 Number of pruned stochastic weights: 281\n",
      ">>> Iteration # 4000: Difference from GP 20350.2524 Number of pruned stochastic weights: 294\n",
      ">>> Iteration # 4100: Difference from GP 20519.0362 Number of pruned stochastic weights: 306\n",
      ">>> Iteration # 4200: Difference from GP 20476.0899 Number of pruned stochastic weights: 314\n",
      ">>> Iteration # 4300: Difference from GP 20727.9101 Number of pruned stochastic weights: 325\n",
      ">>> Iteration # 4400: Difference from GP 19634.4830 Number of pruned stochastic weights: 333\n",
      ">>> Iteration # 4500: Difference from GP 20492.1593 Number of pruned stochastic weights: 344\n",
      ">>> Iteration # 4600: Difference from GP 20365.8005 Number of pruned stochastic weights: 349\n",
      ">>> Iteration # 4700: Difference from GP 19620.4695 Number of pruned stochastic weights: 360\n",
      ">>> Iteration # 4800: Difference from GP 20330.5336 Number of pruned stochastic weights: 362\n",
      ">>> Iteration # 4900: Difference from GP 20169.6925 Number of pruned stochastic weights: 365\n",
      ">>> Iteration # 5000: Difference from GP 20263.7987 Number of pruned stochastic weights: 366\n",
      ">>> Iteration # 5100: Difference from GP 20702.0515 Number of pruned stochastic weights: 373\n",
      ">>> Iteration # 5200: Difference from GP 19938.5590 Number of pruned stochastic weights: 375\n",
      ">>> Iteration # 5300: Difference from GP 20513.4953 Number of pruned stochastic weights: 377\n",
      ">>> Iteration # 5400: Difference from GP 21087.1951 Number of pruned stochastic weights: 381\n",
      ">>> Iteration # 5500: Difference from GP 20741.8574 Number of pruned stochastic weights: 383\n",
      ">>> Iteration # 5600: Difference from GP 20885.3685 Number of pruned stochastic weights: 384\n",
      ">>> Iteration # 5700: Difference from GP 19837.1996 Number of pruned stochastic weights: 384\n",
      ">>> Iteration # 5800: Difference from GP 20294.5548 Number of pruned stochastic weights: 384\n",
      ">>> Iteration # 5900: Difference from GP 20128.8117 Number of pruned stochastic weights: 384\n",
      ">>> Iteration # 6000: Difference from GP 20583.5843 Number of pruned stochastic weights: 384\n",
      "--------------------------------------------------------------------------------\n",
      "Loading split 8 of boston dataset\n",
      ">>> Iteration #   1: Difference from GP 20104.5139 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 100: Difference from GP 23474.0262 Number of pruned stochastic weights: 3\n",
      ">>> Iteration # 200: Difference from GP 20500.1020 Number of pruned stochastic weights: 3\n",
      ">>> Iteration # 300: Difference from GP 20206.0149 Number of pruned stochastic weights: 3\n",
      ">>> Iteration # 400: Difference from GP 19915.4100 Number of pruned stochastic weights: 3\n",
      ">>> Iteration # 500: Difference from GP 19993.7216 Number of pruned stochastic weights: 3\n",
      ">>> Iteration # 600: Difference from GP 20614.0337 Number of pruned stochastic weights: 3\n",
      ">>> Iteration # 700: Difference from GP 19695.4552 Number of pruned stochastic weights: 3\n",
      ">>> Iteration # 800: Difference from GP 20048.1511 Number of pruned stochastic weights: 3\n",
      ">>> Iteration # 900: Difference from GP 20405.8169 Number of pruned stochastic weights: 3\n",
      ">>> Iteration # 1000: Difference from GP 20710.9701 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 1100: Difference from GP 20240.4262 Number of pruned stochastic weights: 5\n",
      ">>> Iteration # 1200: Difference from GP 19834.1575 Number of pruned stochastic weights: 7\n",
      ">>> Iteration # 1300: Difference from GP 20500.4896 Number of pruned stochastic weights: 9\n",
      ">>> Iteration # 1400: Difference from GP 20019.5698 Number of pruned stochastic weights: 21\n",
      ">>> Iteration # 1500: Difference from GP 20621.7755 Number of pruned stochastic weights: 21\n",
      ">>> Iteration # 1600: Difference from GP 20326.1813 Number of pruned stochastic weights: 21\n",
      ">>> Iteration # 1700: Difference from GP 19803.0692 Number of pruned stochastic weights: 21\n",
      ">>> Iteration # 1800: Difference from GP 19354.1606 Number of pruned stochastic weights: 21\n",
      ">>> Iteration # 1900: Difference from GP 20126.8725 Number of pruned stochastic weights: 23\n",
      ">>> Iteration # 2000: Difference from GP 19499.3462 Number of pruned stochastic weights: 25\n",
      ">>> Iteration # 2100: Difference from GP 20200.1798 Number of pruned stochastic weights: 27\n",
      ">>> Iteration # 2200: Difference from GP 20330.2505 Number of pruned stochastic weights: 30\n",
      ">>> Iteration # 2300: Difference from GP 20028.1766 Number of pruned stochastic weights: 43\n",
      ">>> Iteration # 2400: Difference from GP 19890.1582 Number of pruned stochastic weights: 52\n",
      ">>> Iteration # 2500: Difference from GP 20303.2135 Number of pruned stochastic weights: 62\n",
      ">>> Iteration # 2600: Difference from GP 20090.7339 Number of pruned stochastic weights: 67\n",
      ">>> Iteration # 2700: Difference from GP 20595.6725 Number of pruned stochastic weights: 84\n",
      ">>> Iteration # 2800: Difference from GP 19941.2546 Number of pruned stochastic weights: 95\n",
      ">>> Iteration # 2900: Difference from GP 19741.3665 Number of pruned stochastic weights: 107\n",
      ">>> Iteration # 3000: Difference from GP 19926.2917 Number of pruned stochastic weights: 117\n",
      ">>> Iteration # 3100: Difference from GP 20230.0053 Number of pruned stochastic weights: 122\n",
      ">>> Iteration # 3200: Difference from GP 20680.6582 Number of pruned stochastic weights: 133\n",
      ">>> Iteration # 3300: Difference from GP 19986.4292 Number of pruned stochastic weights: 147\n",
      ">>> Iteration # 3400: Difference from GP 20134.1151 Number of pruned stochastic weights: 156\n",
      ">>> Iteration # 3500: Difference from GP 20289.0074 Number of pruned stochastic weights: 167\n",
      ">>> Iteration # 3600: Difference from GP 20453.7445 Number of pruned stochastic weights: 181\n",
      ">>> Iteration # 3700: Difference from GP 20031.5264 Number of pruned stochastic weights: 196\n",
      ">>> Iteration # 3800: Difference from GP 19816.7251 Number of pruned stochastic weights: 202\n",
      ">>> Iteration # 3900: Difference from GP 19639.1556 Number of pruned stochastic weights: 210\n",
      ">>> Iteration # 4000: Difference from GP 20296.0945 Number of pruned stochastic weights: 222\n",
      ">>> Iteration # 4100: Difference from GP 19947.8150 Number of pruned stochastic weights: 225\n",
      ">>> Iteration # 4200: Difference from GP 20027.3558 Number of pruned stochastic weights: 231\n",
      ">>> Iteration # 4300: Difference from GP 20622.0414 Number of pruned stochastic weights: 241\n",
      ">>> Iteration # 4400: Difference from GP 19693.8980 Number of pruned stochastic weights: 249\n",
      ">>> Iteration # 4500: Difference from GP 19719.8159 Number of pruned stochastic weights: 257\n",
      ">>> Iteration # 4600: Difference from GP 20173.0568 Number of pruned stochastic weights: 260\n",
      ">>> Iteration # 4700: Difference from GP 20061.8380 Number of pruned stochastic weights: 267\n",
      ">>> Iteration # 4800: Difference from GP 19734.3312 Number of pruned stochastic weights: 273\n",
      ">>> Iteration # 4900: Difference from GP 20508.0486 Number of pruned stochastic weights: 275\n",
      ">>> Iteration # 5000: Difference from GP 20414.9072 Number of pruned stochastic weights: 278\n",
      ">>> Iteration # 5100: Difference from GP 20462.5019 Number of pruned stochastic weights: 281\n",
      ">>> Iteration # 5200: Difference from GP 20037.1879 Number of pruned stochastic weights: 283\n",
      ">>> Iteration # 5300: Difference from GP 20561.9189 Number of pruned stochastic weights: 286\n",
      ">>> Iteration # 5400: Difference from GP 19973.2247 Number of pruned stochastic weights: 289\n",
      ">>> Iteration # 5500: Difference from GP 20185.6387 Number of pruned stochastic weights: 291\n",
      ">>> Iteration # 5600: Difference from GP 20367.8934 Number of pruned stochastic weights: 292\n",
      ">>> Iteration # 5700: Difference from GP 19519.2934 Number of pruned stochastic weights: 292\n",
      ">>> Iteration # 5800: Difference from GP 20029.2506 Number of pruned stochastic weights: 292\n",
      ">>> Iteration # 5900: Difference from GP 19736.8329 Number of pruned stochastic weights: 292\n",
      ">>> Iteration # 6000: Difference from GP 19746.9919 Number of pruned stochastic weights: 292\n",
      "--------------------------------------------------------------------------------\n",
      "Loading split 9 of boston dataset\n",
      ">>> Iteration #   1: Difference from GP 19934.2027 Number of pruned stochastic weights: 0\n",
      ">>> Iteration # 100: Difference from GP 20344.5315 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 200: Difference from GP 23240.2533 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 300: Difference from GP 20280.1174 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 400: Difference from GP 20035.6060 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 500: Difference from GP 20210.8741 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 600: Difference from GP 19673.9271 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 700: Difference from GP 20187.2096 Number of pruned stochastic weights: 5\n",
      ">>> Iteration # 800: Difference from GP 20854.5364 Number of pruned stochastic weights: 5\n",
      ">>> Iteration # 900: Difference from GP 20020.5206 Number of pruned stochastic weights: 13\n",
      ">>> Iteration # 1000: Difference from GP 20391.6498 Number of pruned stochastic weights: 13\n",
      ">>> Iteration # 1100: Difference from GP 20183.1683 Number of pruned stochastic weights: 13\n",
      ">>> Iteration # 1200: Difference from GP 20232.9965 Number of pruned stochastic weights: 13\n",
      ">>> Iteration # 1300: Difference from GP 20060.8373 Number of pruned stochastic weights: 14\n",
      ">>> Iteration # 1400: Difference from GP 20280.0166 Number of pruned stochastic weights: 17\n",
      ">>> Iteration # 1500: Difference from GP 19713.8703 Number of pruned stochastic weights: 17\n",
      ">>> Iteration # 1600: Difference from GP 20436.8458 Number of pruned stochastic weights: 25\n",
      ">>> Iteration # 1700: Difference from GP 26451.0122 Number of pruned stochastic weights: 40\n",
      ">>> Iteration # 1800: Difference from GP 20015.3983 Number of pruned stochastic weights: 41\n",
      ">>> Iteration # 1900: Difference from GP 20299.9267 Number of pruned stochastic weights: 43\n",
      ">>> Iteration # 2000: Difference from GP 20283.8590 Number of pruned stochastic weights: 46\n",
      ">>> Iteration # 2100: Difference from GP 20788.2601 Number of pruned stochastic weights: 48\n",
      ">>> Iteration # 2200: Difference from GP 20204.2487 Number of pruned stochastic weights: 54\n",
      ">>> Iteration # 2300: Difference from GP 19874.4119 Number of pruned stochastic weights: 60\n",
      ">>> Iteration # 2400: Difference from GP 19911.9248 Number of pruned stochastic weights: 70\n",
      ">>> Iteration # 2500: Difference from GP 20139.3578 Number of pruned stochastic weights: 81\n",
      ">>> Iteration # 2600: Difference from GP 20169.6750 Number of pruned stochastic weights: 93\n",
      ">>> Iteration # 2700: Difference from GP 19855.9925 Number of pruned stochastic weights: 109\n",
      ">>> Iteration # 2800: Difference from GP 20062.0191 Number of pruned stochastic weights: 117\n",
      ">>> Iteration # 2900: Difference from GP 19966.6584 Number of pruned stochastic weights: 131\n",
      ">>> Iteration # 3000: Difference from GP 20199.3237 Number of pruned stochastic weights: 145\n",
      ">>> Iteration # 3100: Difference from GP 20454.0178 Number of pruned stochastic weights: 160\n",
      ">>> Iteration # 3200: Difference from GP 20137.9691 Number of pruned stochastic weights: 178\n",
      ">>> Iteration # 3300: Difference from GP 20609.2495 Number of pruned stochastic weights: 190\n",
      ">>> Iteration # 3400: Difference from GP 20084.5010 Number of pruned stochastic weights: 202\n",
      ">>> Iteration # 3500: Difference from GP 20190.0013 Number of pruned stochastic weights: 217\n",
      ">>> Iteration # 3600: Difference from GP 19477.2223 Number of pruned stochastic weights: 229\n",
      ">>> Iteration # 3700: Difference from GP 20269.0533 Number of pruned stochastic weights: 242\n",
      ">>> Iteration # 3800: Difference from GP 20024.3307 Number of pruned stochastic weights: 251\n",
      ">>> Iteration # 3900: Difference from GP 20220.1049 Number of pruned stochastic weights: 261\n",
      ">>> Iteration # 4000: Difference from GP 19902.5139 Number of pruned stochastic weights: 278\n",
      ">>> Iteration # 4100: Difference from GP 20365.8361 Number of pruned stochastic weights: 288\n",
      ">>> Iteration # 4200: Difference from GP 20293.2630 Number of pruned stochastic weights: 294\n",
      ">>> Iteration # 4300: Difference from GP 20759.2904 Number of pruned stochastic weights: 305\n",
      ">>> Iteration # 4400: Difference from GP 20220.7090 Number of pruned stochastic weights: 308\n",
      ">>> Iteration # 4500: Difference from GP 19749.0066 Number of pruned stochastic weights: 320\n",
      ">>> Iteration # 4600: Difference from GP 19852.3420 Number of pruned stochastic weights: 325\n",
      ">>> Iteration # 4700: Difference from GP 20304.7207 Number of pruned stochastic weights: 332\n",
      ">>> Iteration # 4800: Difference from GP 19684.5948 Number of pruned stochastic weights: 337\n",
      ">>> Iteration # 4900: Difference from GP 19545.1708 Number of pruned stochastic weights: 343\n",
      ">>> Iteration # 5000: Difference from GP 19838.2447 Number of pruned stochastic weights: 347\n",
      ">>> Iteration # 5100: Difference from GP 20253.6370 Number of pruned stochastic weights: 353\n",
      ">>> Iteration # 5200: Difference from GP 20094.0565 Number of pruned stochastic weights: 356\n",
      ">>> Iteration # 5300: Difference from GP 20523.7578 Number of pruned stochastic weights: 356\n",
      ">>> Iteration # 5400: Difference from GP 19895.3119 Number of pruned stochastic weights: 358\n",
      ">>> Iteration # 5500: Difference from GP 19422.8876 Number of pruned stochastic weights: 360\n",
      ">>> Iteration # 5600: Difference from GP 19885.9975 Number of pruned stochastic weights: 361\n",
      ">>> Iteration # 5700: Difference from GP 19471.6000 Number of pruned stochastic weights: 361\n",
      ">>> Iteration # 5800: Difference from GP 19295.1527 Number of pruned stochastic weights: 362\n",
      ">>> Iteration # 5900: Difference from GP 21264.6244 Number of pruned stochastic weights: 362\n",
      ">>> Iteration # 6000: Difference from GP 20262.4897 Number of pruned stochastic weights: 362\n",
      "--------------------------------------------------------------------------------\n",
      "Loading split 10 of boston dataset\n",
      ">>> Iteration #   1: Difference from GP 19844.7809 Number of pruned stochastic weights: 1\n",
      ">>> Iteration # 100: Difference from GP 45324.4485 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 200: Difference from GP 20316.0606 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 300: Difference from GP 20478.8793 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 400: Difference from GP 20864.1158 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 500: Difference from GP 21397.9961 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 600: Difference from GP 20450.7750 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 700: Difference from GP 19915.0242 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 800: Difference from GP 20463.2847 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 900: Difference from GP 20351.1910 Number of pruned stochastic weights: 4\n",
      ">>> Iteration # 1000: Difference from GP 20694.2956 Number of pruned stochastic weights: 5\n",
      ">>> Iteration # 1100: Difference from GP 19736.1298 Number of pruned stochastic weights: 5\n",
      ">>> Iteration # 1200: Difference from GP 20137.2347 Number of pruned stochastic weights: 6\n",
      ">>> Iteration # 1300: Difference from GP 20653.3685 Number of pruned stochastic weights: 9\n",
      ">>> Iteration # 1400: Difference from GP 19593.4921 Number of pruned stochastic weights: 9\n",
      ">>> Iteration # 1500: Difference from GP 21069.1669 Number of pruned stochastic weights: 21\n",
      ">>> Iteration # 1600: Difference from GP 20629.9687 Number of pruned stochastic weights: 27\n",
      ">>> Iteration # 1700: Difference from GP 20066.5805 Number of pruned stochastic weights: 29\n",
      ">>> Iteration # 1800: Difference from GP 19850.8686 Number of pruned stochastic weights: 29\n",
      ">>> Iteration # 1900: Difference from GP 20565.7034 Number of pruned stochastic weights: 34\n",
      ">>> Iteration # 2000: Difference from GP 20457.7000 Number of pruned stochastic weights: 35\n",
      ">>> Iteration # 2100: Difference from GP 20267.4287 Number of pruned stochastic weights: 41\n",
      ">>> Iteration # 2200: Difference from GP 21192.3114 Number of pruned stochastic weights: 57\n",
      ">>> Iteration # 2300: Difference from GP 20132.9681 Number of pruned stochastic weights: 71\n",
      ">>> Iteration # 2400: Difference from GP 20883.2190 Number of pruned stochastic weights: 84\n",
      ">>> Iteration # 2500: Difference from GP 20559.3570 Number of pruned stochastic weights: 95\n",
      ">>> Iteration # 2600: Difference from GP 20387.0880 Number of pruned stochastic weights: 107\n",
      ">>> Iteration # 2700: Difference from GP 20393.8978 Number of pruned stochastic weights: 125\n",
      ">>> Iteration # 2800: Difference from GP 20395.0697 Number of pruned stochastic weights: 138\n",
      ">>> Iteration # 2900: Difference from GP 20487.7441 Number of pruned stochastic weights: 159\n",
      ">>> Iteration # 3000: Difference from GP 20429.0988 Number of pruned stochastic weights: 179\n",
      ">>> Iteration # 3100: Difference from GP 19800.6737 Number of pruned stochastic weights: 197\n",
      ">>> Iteration # 3200: Difference from GP 20327.6150 Number of pruned stochastic weights: 210\n",
      ">>> Iteration # 3300: Difference from GP 20665.3720 Number of pruned stochastic weights: 226\n",
      ">>> Iteration # 3400: Difference from GP 20012.1695 Number of pruned stochastic weights: 242\n",
      ">>> Iteration # 3500: Difference from GP 20132.0188 Number of pruned stochastic weights: 259\n",
      ">>> Iteration # 3600: Difference from GP 20217.6492 Number of pruned stochastic weights: 276\n",
      ">>> Iteration # 3700: Difference from GP 20273.9856 Number of pruned stochastic weights: 283\n",
      ">>> Iteration # 3800: Difference from GP 20758.1073 Number of pruned stochastic weights: 293\n",
      ">>> Iteration # 3900: Difference from GP 20116.3391 Number of pruned stochastic weights: 301\n",
      ">>> Iteration # 4000: Difference from GP 20159.7881 Number of pruned stochastic weights: 317\n",
      ">>> Iteration # 4100: Difference from GP 20096.7305 Number of pruned stochastic weights: 327\n",
      ">>> Iteration # 4200: Difference from GP 20818.5507 Number of pruned stochastic weights: 337\n",
      ">>> Iteration # 4300: Difference from GP 20756.4029 Number of pruned stochastic weights: 346\n",
      ">>> Iteration # 4400: Difference from GP 20468.4116 Number of pruned stochastic weights: 349\n",
      ">>> Iteration # 4500: Difference from GP 20405.4822 Number of pruned stochastic weights: 358\n",
      ">>> Iteration # 4600: Difference from GP 19473.4713 Number of pruned stochastic weights: 367\n",
      ">>> Iteration # 4700: Difference from GP 20480.5342 Number of pruned stochastic weights: 371\n",
      ">>> Iteration # 4800: Difference from GP 20714.2490 Number of pruned stochastic weights: 373\n",
      ">>> Iteration # 4900: Difference from GP 20151.8569 Number of pruned stochastic weights: 375\n",
      ">>> Iteration # 5000: Difference from GP 20452.0974 Number of pruned stochastic weights: 379\n",
      ">>> Iteration # 5100: Difference from GP 20310.6975 Number of pruned stochastic weights: 380\n",
      ">>> Iteration # 5200: Difference from GP 20045.3929 Number of pruned stochastic weights: 382\n",
      ">>> Iteration # 5300: Difference from GP 20230.2123 Number of pruned stochastic weights: 383\n",
      ">>> Iteration # 5400: Difference from GP 20402.5916 Number of pruned stochastic weights: 384\n",
      ">>> Iteration # 5500: Difference from GP 20613.9128 Number of pruned stochastic weights: 385\n",
      ">>> Iteration # 5600: Difference from GP 20473.8823 Number of pruned stochastic weights: 385\n",
      ">>> Iteration # 5700: Difference from GP 20191.0193 Number of pruned stochastic weights: 385\n",
      ">>> Iteration # 5800: Difference from GP 20235.5983 Number of pruned stochastic weights: 385\n",
      ">>> Iteration # 5900: Difference from GP 20255.9088 Number of pruned stochastic weights: 385\n",
      ">>> Iteration # 6000: Difference from GP 20816.5618 Number of pruned stochastic weights: 385\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "masks_list = []\n",
    "for split_id in range(n_splits):\n",
    "    print(\"Loading split {} of {} dataset\".format(split_id+1, dataset))\n",
    "    # Load the dataset\n",
    "    saved_dir = os.path.join(out_dir, str(split_id))\n",
    "    X_train, y_train, X_test, y_test = util.load_uci_data(\n",
    "            data_dir, split_id, dataset)\n",
    "    X_train_, y_train_, X_test_, y_test_, y_mean, y_std = normalize_data(\n",
    "            X_train, y_train, X_test, y_test)\n",
    "    x_min, x_max = get_input_range(X_train_, X_test_)\n",
    "    input_dim, output_dim = int(X_train.shape[-1]), 1\n",
    "    # Initialize the measurement set generator\n",
    "    rand_generator = MeasureSetGenerator(X_train_, x_min, x_max, 0.7)\n",
    "    \n",
    "    # Initialize the mean and covariance function of the target hierarchical GP prior\n",
    "    mean = mean_functions.Zero()\n",
    "    \n",
    "    lengthscale = math.sqrt(2. * input_dim)\n",
    "    variance = 1.\n",
    "    # BEFORE RBF\n",
    "    kernel = kernels.Exponential(input_dim=input_dim,\n",
    "                         lengthscales=torch.tensor([lengthscale], dtype=torch.double),\n",
    "                         variance=torch.tensor([variance], dtype=torch.double), ARD=True)\n",
    "\n",
    "    # Place hyper-priors on lengthscales and variances\n",
    "    kernel.lengthscales.prior = LogNormal(\n",
    "            torch.ones([input_dim]) * math.log(lengthscale),\n",
    "            torch.ones([input_dim]) * 1.)\n",
    "    kernel.variance.prior = LogNormal(\n",
    "            torch.ones([1]) * 0.1,\n",
    "            torch.ones([1]) * 1.)\n",
    "    kernel = kernel.to(device)\n",
    "    # Initialize tunable MLP prior\n",
    "    hidden_dims = [n_units] * n_hidden\n",
    "    mlp_reparam = FactorizedGaussianMLPReparameterization(input_dim, output_dim,\n",
    "        hidden_dims, D = D, activation_fn=activation_fn, scaled_variance=True, device=device)\n",
    "    mlp_reparam = mlp_reparam.to(device)\n",
    "    # Perform optimization\n",
    "    mapper = PriorOptimisationMapper(out_dir=saved_dir, device=device, kernel = kernel).to(device)\n",
    "    p_hist, loss_hist = mapper.optimize(mlp_reparam, rand_generator, output_dim=output_dim, **prior_opt_configurations)\n",
    "    path = os.path.join(saved_dir, \"loss_values.log\")\n",
    "    if not os.path.isfile(saved_dir):\n",
    "        os.makedirs(saved_dir, exist_ok=True)\n",
    "    np.savetxt(path, loss_hist, fmt='%.6e')\n",
    "    path = os.path.join(saved_dir, \"pruned_values.log\")\n",
    "    if not os.path.isfile(saved_dir):\n",
    "        os.makedirs(saved_dir, exist_ok=True)\n",
    "    np.savetxt(path, p_hist, fmt='%.6e')\n",
    "    print(\"----\" * 20)\n",
    "    masks_list.append(mlp_reparam.get_det_masks())\n",
    "# Save the masks\n",
    "with open(os.path.join(out_dir, \"masks_list.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(masks_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x350 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the convergence\n",
    "lossdist_data = []\n",
    "pruned_vals_data = []\n",
    "for i in range(0, n_splits):\n",
    "    lossdist_file = os.path.join(out_dir, str(i), \"loss_values.log\")\n",
    "    pruned_vals_file = os.path.join(out_dir, str(i), \"pruned_values.log\")\n",
    "    lossdist_data.append(np.loadtxt(lossdist_file))\n",
    "    pruned_vals_data.append(np.loadtxt(pruned_vals_file))\n",
    "\n",
    "lossdist_vals = np.stack(lossdist_data)\n",
    "pruned_vals = np.stack(pruned_vals_data)\n",
    "indices = np.arange(prior_opt_configurations[\"num_iters\"])[::60]\n",
    "mean_loss = lossdist_vals.mean(0)\n",
    "std_loss = lossdist_vals.std(0)\n",
    "mean_pruned = pruned_vals.mean(0)\n",
    "std_pruned = pruned_vals.std(0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 3.5))  \n",
    "axes[0].plot(indices, mean_pruned[indices], \"ko\", ms=4)\n",
    "axes[0].fill_between(indices, mean_pruned[indices] - std_pruned[indices],\n",
    "                 mean_pruned[indices] + std_pruned[indices], alpha=0.18, color=\"k\")\n",
    "axes[0].set_ylabel(\"Number pruned weights\")\n",
    "axes[0].set_xlabel(\"Iteration\")\n",
    "axes[0].yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "axes[1].plot(indices, mean_loss[indices], \"-ko\", ms=4)\n",
    "axes[1].fill_between(indices, mean_loss[indices]-std_loss[indices],\n",
    "                     mean_loss[indices]+std_loss[indices], alpha = 0.18, color = \"k\")\n",
    "axes[1].set_ylabel(\"Distance from GP\")\n",
    "axes[1].set_xlabel(\"Iteration\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Posterior Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the SGHMC sampler\n",
    "sampling_configs = {\n",
    "    \"batch_size\": 32,                 # Mini-batch size\n",
    "    \"num_samples\": 40,                # Total number of samples for each chain \n",
    "    \"n_discarded\": 10,                # Number of the first samples to be discared for each chain\n",
    "    \"num_burn_in_steps\": 2000, #2000        # Number of burn-in steps\n",
    "    \"keep_every\": 2000,    #2000         # Thinning interval\n",
    "    \"lr\": 3e-2,    #1e-1                   # Step size\n",
    "    \"num_chains\": 4,                  # Number of chains\n",
    "    \"mdecay\": 1e-2, #5e-2                  # Momentum coefficient\n",
    "    \"print_every_n_samples\": 5\n",
    "}\n",
    "# Load the masks\n",
    "with open(os.path.join(out_dir, \"masks_list.pkl\"), \"rb\") as f:\n",
    "    masks_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading split 1 of boston dataset\n",
      "Chain: 1\n",
      "Samples #     5 : NLL =  2.1688e+00 RMSE = 1.4270e+00 Pruned weights: 106\n",
      "Samples #    10 : NLL =  2.1850e+00 RMSE = 1.4096e+00 Pruned weights: 117\n",
      "Samples #    15 : NLL =  2.1927e+00 RMSE = 1.4198e+00 Pruned weights: 124\n",
      "Samples #    20 : NLL =  2.2026e+00 RMSE = 1.4438e+00 Pruned weights: 129\n",
      "Samples #    25 : NLL =  2.2145e+00 RMSE = 1.4893e+00 Pruned weights: 138\n",
      "Samples #    30 : NLL =  2.2246e+00 RMSE = 1.5190e+00 Pruned weights: 140\n",
      "Chain: 2\n",
      "Samples #    35 : NLL =  2.2416e+00 RMSE = 1.5630e+00 Pruned weights: 163\n",
      "Samples #    40 : NLL =  2.2534e+00 RMSE = 1.6104e+00 Pruned weights: 164\n",
      "Samples #    45 : NLL =  2.2640e+00 RMSE = 1.6531e+00 Pruned weights: 167\n",
      "Samples #    50 : NLL =  2.2717e+00 RMSE = 1.6982e+00 Pruned weights: 168\n",
      "Samples #    55 : NLL =  2.2783e+00 RMSE = 1.7323e+00 Pruned weights: 170\n",
      "Samples #    60 : NLL =  2.2839e+00 RMSE = 1.7722e+00 Pruned weights: 170\n",
      "Chain: 3\n",
      "Samples #    65 : NLL =  2.2913e+00 RMSE = 1.8203e+00 Pruned weights: 176\n",
      "Samples #    70 : NLL =  2.2983e+00 RMSE = 1.8710e+00 Pruned weights: 177\n",
      "Samples #    75 : NLL =  2.3053e+00 RMSE = 1.9192e+00 Pruned weights: 177\n"
     ]
    }
   ],
   "source": [
    "results = {\"rmse\": [], \"nll\": []}\n",
    "\n",
    "for split_id in range(n_splits):\n",
    "    print(\"Loading split {} of {} dataset\".format(split_id+1, dataset))\n",
    "    saved_dir = os.path.join(out_dir, str(split_id))\n",
    "    \n",
    "    # Load the dataset\n",
    "    X_train, y_train, X_test, y_test = util.load_uci_data(\n",
    "            data_dir, split_id, dataset)\n",
    "    input_dim, output_dim = int(X_train.shape[-1]), 1\n",
    "    # Initialize the neural network and likelihood modules\n",
    "    weight_mask, bias_mask = masks_list[split_id]\n",
    "    net = MLPMasked(input_dim, output_dim, [n_units] * n_hidden, activation_fn, weight_mask, \n",
    "                    bias_mask, D = D, device = device)\n",
    "    net = net.to(device)\n",
    "    likelihood = LikGaussian(noise_var)\n",
    "    \n",
    "    # Load the optimized prior\n",
    "    ckpt_path = os.path.join(out_dir, str(split_id), \"ckpts\", \"it-{}.ckpt\".format(prior_opt_configurations[\"num_iters\"]))\n",
    "    prior = OptimGaussianPrior(ckpt_path)\n",
    "    \n",
    "    # Initialize bayesian neural network with SGHMC sampler\n",
    "    saved_dir = os.path.join(out_dir, str(split_id))\n",
    "    bayes_net = RegressionNetMasked(net, likelihood, prior, saved_dir, n_gpu=n_gpu)\n",
    "    \n",
    "    # Start sampling\n",
    "    bayes_net.sample_multi_chains(X_train, y_train, **sampling_configs)\n",
    "    pred_mean, pred_var, preds, raw_preds = bayes_net.predict(X_test, True, True)\n",
    "    r_hat = compute_rhat_regression(raw_preds, sampling_configs[\"num_chains\"])\n",
    "    print(\"R-hat: mean {:.4f} std {:.4f}\".format(float(r_hat.mean()), float(r_hat.std())))\n",
    "\n",
    "    rmse = uncertainty_metrics.rmse(pred_mean, y_test)\n",
    "    nll = uncertainty_metrics.gaussian_nll(y_test, pred_mean, pred_var)\n",
    "    print(\"> RMSE = {:.4f} | NLL = {:.4f}\".format(rmse, nll))\n",
    "    results['rmse'].append(rmse)\n",
    "    results['nll'].append(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv(os.path.join(out_dir, \"optim_results.csv\"), sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_csv(os.path.join(out_dir, \"optim_results.csv\"), sep = \"\\t\")\n",
    "print(\"Final result:\")\n",
    "print(\"> RMSE: mean {:.4e}; std {:.4e} | NLL: mean {:.4e} std {:.4e}\".format(\n",
    "        float(result_df['rmse'].mean()), float(result_df['rmse'].std()),\n",
    "        float(result_df['nll'].mean()), float(result_df['nll'].std())))\n",
    "print(\"\\nFinal results benchmark:\")\n",
    "print(\"> RMSE: mean 2.7970e+00; std 9.1485e-01 | NLL: mean 2.4699e+00 std 1.4601e-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
